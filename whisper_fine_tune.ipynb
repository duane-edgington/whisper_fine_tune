{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "872c730394c74258a2c6407a547b6785": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_241602e9c8b346b396558223e49e3175",
              "IPY_MODEL_11fd1e0c700745a39bdc15fd53a362f5",
              "IPY_MODEL_f67c9f8a87c14acaa69f52c77b575581"
            ],
            "layout": "IPY_MODEL_08a7d0dd91c6454bb97d107a5366a4c5"
          }
        },
        "241602e9c8b346b396558223e49e3175": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1b144dbb54d413485b35dce6589d32f",
            "placeholder": "​",
            "style": "IPY_MODEL_c6ca4cb24a7e4e0a9e38d814c7c3410c",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "11fd1e0c700745a39bdc15fd53a362f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_429e27a3d03e444b87619577dccead13",
            "max": 184990,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6986abae03e74d4aa6f9f18a32c98ce2",
            "value": 184990
          }
        },
        "f67c9f8a87c14acaa69f52c77b575581": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d97966d454994f36bd9f2432644900cf",
            "placeholder": "​",
            "style": "IPY_MODEL_325191e8d69b4d56b09f7ff99186664c",
            "value": " 185k/185k [00:00&lt;00:00, 2.40MB/s]"
          }
        },
        "08a7d0dd91c6454bb97d107a5366a4c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1b144dbb54d413485b35dce6589d32f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6ca4cb24a7e4e0a9e38d814c7c3410c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "429e27a3d03e444b87619577dccead13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6986abae03e74d4aa6f9f18a32c98ce2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d97966d454994f36bd9f2432644900cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "325191e8d69b4d56b09f7ff99186664c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a770243d88a0479e8561078034a089ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93aa175a7cb245c1bdb982a739a7a347",
              "IPY_MODEL_c358f66f72a844c7bd7eb6ce030df385",
              "IPY_MODEL_5954543e14ab4970b9be69ef1734b45f"
            ],
            "layout": "IPY_MODEL_ae4d9eaaba7f4a8fa6b774edcc3cb234"
          }
        },
        "93aa175a7cb245c1bdb982a739a7a347": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_697e0bb66de34124936ecd825d6f8ff5",
            "placeholder": "​",
            "style": "IPY_MODEL_2f788e116c164aeca3d432f5e4830818",
            "value": "config.json: 100%"
          }
        },
        "c358f66f72a844c7bd7eb6ce030df385": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1ebd7414a0a4d5eb5c5a490be9a0b58",
            "max": 1967,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36b0e1a65b1a42ffb07e60646f636f5c",
            "value": 1967
          }
        },
        "5954543e14ab4970b9be69ef1734b45f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af1abfad565a473faf6ef428add84328",
            "placeholder": "​",
            "style": "IPY_MODEL_083e7649258042c8ae69ef31303db34a",
            "value": " 1.97k/1.97k [00:00&lt;00:00, 31.5kB/s]"
          }
        },
        "ae4d9eaaba7f4a8fa6b774edcc3cb234": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "697e0bb66de34124936ecd825d6f8ff5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f788e116c164aeca3d432f5e4830818": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1ebd7414a0a4d5eb5c5a490be9a0b58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36b0e1a65b1a42ffb07e60646f636f5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af1abfad565a473faf6ef428add84328": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "083e7649258042c8ae69ef31303db34a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e514c530132246ce9adc9637fbdefba5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d78b5a823c94c3b9be653f763577c07",
              "IPY_MODEL_d48c8bc5db2042f08760785bcbb87990",
              "IPY_MODEL_d3fe243dd59d4ff5bbf08e55754e75c8"
            ],
            "layout": "IPY_MODEL_c370a026816e4708bda53ac34267c184"
          }
        },
        "3d78b5a823c94c3b9be653f763577c07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f456651751314476837dd8346f13f523",
            "placeholder": "​",
            "style": "IPY_MODEL_f8996ca365d34e0cbe32c866bb39db79",
            "value": "model.safetensors: 100%"
          }
        },
        "d48c8bc5db2042f08760785bcbb87990": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1327dab11d0948f6aba24bd8506b4917",
            "max": 966995080,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_51844eb832b64d0a8020c80d91ff11eb",
            "value": 966995080
          }
        },
        "d3fe243dd59d4ff5bbf08e55754e75c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59f2cbb0f9044a5fbe224cc93e015273",
            "placeholder": "​",
            "style": "IPY_MODEL_6f2ce970ff324b35851f41ee2355cfae",
            "value": " 967M/967M [00:08&lt;00:00, 198MB/s]"
          }
        },
        "c370a026816e4708bda53ac34267c184": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f456651751314476837dd8346f13f523": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8996ca365d34e0cbe32c866bb39db79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1327dab11d0948f6aba24bd8506b4917": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51844eb832b64d0a8020c80d91ff11eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "59f2cbb0f9044a5fbe224cc93e015273": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f2ce970ff324b35851f41ee2355cfae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Fine-Tuning OpenAI Whisper Model for Audio Classification in PyTorch\n",
        "https://www.daniweb.com/programming/computer-science/tutorials/540802/fine-tuning-openai-whisper-model-for-audio-classification-in-pytorch#post2294618\n",
        "##Introduction\n",
        "In a previous article, I explained how to fine-tune the vision transformer model for image classification in PyTorch. https://www.daniweb.com/programming/computer-science/tutorials/540749/fine-tuning-vision-transformer-for-image-classification-in-pytorch\n",
        "\n",
        "In this article, I will explain how to fine-tune the pre-trained OpenAI Whisper model for audio classification in PyTorch.\n",
        "\n",
        "Audio classification is an important task that can be applied in various scenarios, such as speech dialogue detection, sentiment analysis, music genre recognition, environmental sound identification, etc.\n",
        "\n",
        "OpenAI Whisper is an excellent model for audio classification that achieved state-of-the-art results on several benchmarks. It is based on the transformer architecture and uses self-attention to process audio inputs. OpenAI Whisper can recognize speech and audio from different languages, accents, and domains with high accuracy and robustness.\n",
        "\n",
        "In this article, you will see how to classify various sounds by fine-tuning the OpenAI Whisper model from Hugging Face in the PyTorch deep learning library. You will learn how to load the pre-trained model, prepare a custom audio dataset, train the model on the dataset, and evaluate the model performance. Let’s get started!\n"
      ],
      "metadata": {
        "id": "huSKt13HfwXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing and Importing Required Libraries\n",
        "You will need to install the Hugging Face Transformers library to run scripts in this article.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZIFlIPM-gZby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install accelerate -U\n",
        "! pip install datasets transformers[sentencepiece]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sVQ6EvaBgxNN",
        "outputId": "df7737af-c240-4fc7-9bb2-914b429b2691"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.24.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.19.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup google drive"
      ],
      "metadata": {
        "id": "s4YrvHyo4GZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "cN4Hf7dt4QSE",
        "outputId": "8c17b77e-e5e5-48e4-ca33-b6b520966d2b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/UrbanSound8K"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "IelP7u0g4eb8",
        "outputId": "3f869511-6fd1-42e4-b8e8-47f564650a91"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_model_10epochs.bin  best_model.pt\tfold3  fold7\t   state_dict_10epochs\n",
            "best_model_10epochs.pt\t fold1\t\tfold4  fold8\t   UrbanSound8K.csv\n",
            "best_model.bin\t\t fold10\t\tfold5  fold9\t   UrbanSound8K.zip\n",
            "best_model_dict.pt\t fold2\t\tfold6  state_dict\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following script imports the Python libraries and modules required to execute Python codes in this article."
      ],
      "metadata": {
        "id": "Gxp7loAIg0S4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from datasets import load_dataset, DatasetDict,  Audio\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import librosa\n",
        "import io\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
        "from transformers import WhisperModel, WhisperFeatureExtractor, AdamW\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n"
      ],
      "metadata": {
        "id": "WY5NqIaJxljk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display torch version and test if GPU is active\n",
        "torch.__version__, torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "lRMVLOFFdPoH",
        "outputId": "6d9dad94-9037-4897-cb92-8d2fa9ef4bac"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('2.4.0+cu121', True)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "slPfTKeS8Zy1",
        "outputId": "5fe353da-febc-4d1d-ed1b-c25a6d46a823"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Sep 11 21:35:09 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing the datasets\n",
        "This article will use the UrbanSound8K dataset from Kaggle. https://www.kaggle.com/datasets/chrisfilo/urbansound8k The dataset consists of audio files containing ten different sound categories. The audio files are located in 10 different folders. It is important to note that each folder may contain audio files belonging to all the categories. A CSV file is also downloaded with the dataset, containing details of each audio file.\n",
        "\n",
        "The following script imports the CSV file into a Pandas DataFrame. The original dataset contains more than 8 thousand records. However, for the sake of experiments in this article, I randomly selected 2000 records."
      ],
      "metadata": {
        "id": "ui4ixUVFxzOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_df = pd.read_csv(r\"/content/drive/MyDrive/UrbanSound8K/UrbanSound8K.csv\")\n",
        "#audio_df = audio_df.sample(n=2000, random_state=42) ## 8732 total samples available\n",
        "audio_df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Fq_YfcoNzY7X",
        "outputId": "7ad10bb7-8b08-44e5-bdd0-07e08ce9f977"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
              "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
              "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
              "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
              "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
              "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
              "\n",
              "              class  \n",
              "0          dog_bark  \n",
              "1  children_playing  \n",
              "2  children_playing  \n",
              "3  children_playing  \n",
              "4  children_playing  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-105ba39b-48fd-40bf-ab1c-266c130683a4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>slice_file_name</th>\n",
              "      <th>fsID</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>salience</th>\n",
              "      <th>fold</th>\n",
              "      <th>classID</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100032-3-0-0.wav</td>\n",
              "      <td>100032</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.317551</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>dog_bark</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100263-2-0-117.wav</td>\n",
              "      <td>100263</td>\n",
              "      <td>58.5</td>\n",
              "      <td>62.500000</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>children_playing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100263-2-0-121.wav</td>\n",
              "      <td>100263</td>\n",
              "      <td>60.5</td>\n",
              "      <td>64.500000</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>children_playing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100263-2-0-126.wav</td>\n",
              "      <td>100263</td>\n",
              "      <td>63.0</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>children_playing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100263-2-0-137.wav</td>\n",
              "      <td>100263</td>\n",
              "      <td>68.5</td>\n",
              "      <td>72.500000</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>children_playing</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-105ba39b-48fd-40bf-ab1c-266c130683a4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-105ba39b-48fd-40bf-ab1c-266c130683a4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-105ba39b-48fd-40bf-ab1c-266c130683a4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-eac31566-ea78-4697-aba0-09fb0cba264e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-eac31566-ea78-4697-aba0-09fb0cba264e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-eac31566-ea78-4697-aba0-09fb0cba264e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "audio_df",
              "summary": "{\n  \"name\": \"audio_df\",\n  \"rows\": 8732,\n  \"fields\": [\n    {\n      \"column\": \"slice_file_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8732,\n        \"samples\": [\n          \"54898-8-0-2.wav\",\n          \"172338-9-0-7.wav\",\n          \"95562-4-3-0.wav\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fsID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57991,\n        \"min\": 344,\n        \"max\": 209992,\n        \"num_unique_values\": 1297,\n        \"samples\": [\n          180257,\n          157940,\n          20015\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 74.29212630755421,\n        \"min\": 0.0,\n        \"max\": 600.125356,\n        \"num_unique_values\": 4878,\n        \"samples\": [\n          10.038318,\n          5.711988,\n          4.634753\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"end\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 74.36966935075833,\n        \"min\": 0.105962,\n        \"max\": 604.125356,\n        \"num_unique_values\": 5020,\n        \"samples\": [\n          13.168347,\n          2.653802,\n          10.286916\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"salience\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fold\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 10,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          3,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"classID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 9,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          7,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"jackhammer\",\n          \"children_playing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![9518a5d86da9dc921a2b78710e2d7fa7.webp](data:image/webp;base64,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)"
      ],
      "metadata": {
        "id": "mLEZRgHTzJvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The slice_file_name column in the above DataFrame contains the audio file name. The classID and class columns contain corresponding class IDs and names.\n",
        "\n",
        "Let's print the class distribution."
      ],
      "metadata": {
        "id": "vSvDfYyn1bOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_df[\"class\"].value_counts()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "E95SqPGfz7NU",
        "outputId": "2a9c69a5-40f9-4efe-dbc0-8dc9d5618721"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "class\n",
              "dog_bark            1000\n",
              "children_playing    1000\n",
              "air_conditioner     1000\n",
              "street_music        1000\n",
              "engine_idling       1000\n",
              "jackhammer          1000\n",
              "drilling            1000\n",
              "siren                929\n",
              "car_horn             429\n",
              "gun_shot             374\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>class</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>dog_bark</th>\n",
              "      <td>1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>children_playing</th>\n",
              "      <td>1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>air_conditioner</th>\n",
              "      <td>1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>street_music</th>\n",
              "      <td>1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>engine_idling</th>\n",
              "      <td>1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jackhammer</th>\n",
              "      <td>1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>drilling</th>\n",
              "      <td>1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>siren</th>\n",
              "      <td>929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>car_horn</th>\n",
              "      <td>429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gun_shot</th>\n",
              "      <td>374</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see different sound categories in the dataset, e.g., street music, dog bark, siren, etc.\n",
        "\n",
        "Next, we will create a column in the Pandas DataFrame containing the audio files' full path. To do so, we will first write a method that creates a dictionary mapping the audio file names to their corresponding full paths. Using this method, we will populate the full_path column in the audio_df DataFrame with the full paths of the audio files.\n",
        "\n",
        "The get_all_full_paths() method in the following script returns a dictionary that maps the audio file names to their corresponding full paths."
      ],
      "metadata": {
        "id": "8UQXlxnY_Boe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_full_paths(parent_directory):\n",
        "  # List to store file paths\n",
        "  audio_file_paths = []\n",
        "\n",
        "  # Iterate through audio folders (assuming they are named fold1, fold2, ..., fold10)\n",
        "  for folder_name in range(1, 11):\n",
        "      folder_path = os.path.join(parent_directory, 'fold{}'.format(folder_name))\n",
        "      # Iterate through files in the current folder and add their paths to the list\n",
        "      for filename in os.listdir(folder_path):\n",
        "          if filename.endswith('.wav'):  # Assuming your audio files have .wav extension\n",
        "              file_path = os.path.join(folder_path, filename)\n",
        "              audio_file_paths.append(file_path)\n",
        "\n",
        "  # Create a dictionary to map base name to full_path\n",
        "  file_path_dict = {os.path.basename(path): path for path in audio_file_paths}\n",
        "  return file_path_dict\n",
        "\n",
        "audio_files_directory = '/content/drive/MyDrive/UrbanSound8K/'\n",
        "file_path_dict = get_all_full_paths(audio_files_directory)\n"
      ],
      "metadata": {
        "id": "h-xbIIPH_GiJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list the wave files base names and full path directories\n",
        "#file_path_dict"
      ],
      "metadata": {
        "id": "xTDPQtyC_JqV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will define the get_single_full_path() method, which accepts the file name as a parameter and returns the corresponding full path from the dictionary returned by the get_all_full_paths() method. Subsequently, we will create a new column full_path in the audio_df DataFrame and use it's apply() method to store full paths in the full_path column."
      ],
      "metadata": {
        "id": "RnkocPNzAirB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_single_full_path(slice_file_name):\n",
        "    return file_path_dict.get(slice_file_name)\n",
        "\n",
        "# Add 'full_path' column to the DataFrame\n",
        "audio_df['full_path'] = audio_df['slice_file_name'].apply(get_single_full_path)\n",
        "audio_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0XJ2dPPTAmrS",
        "outputId": "9eb0134f-fac2-4e66-dc5b-085bdcf29ca5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
              "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
              "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
              "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
              "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
              "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
              "\n",
              "              class                                          full_path  \n",
              "0          dog_bark  /content/drive/MyDrive/UrbanSound8K/fold5/1000...  \n",
              "1  children_playing  /content/drive/MyDrive/UrbanSound8K/fold5/1002...  \n",
              "2  children_playing  /content/drive/MyDrive/UrbanSound8K/fold5/1002...  \n",
              "3  children_playing  /content/drive/MyDrive/UrbanSound8K/fold5/1002...  \n",
              "4  children_playing  /content/drive/MyDrive/UrbanSound8K/fold5/1002...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b8fec5c5-5da2-4bd1-86c5-93fe19e92f97\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>slice_file_name</th>\n",
              "      <th>fsID</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>salience</th>\n",
              "      <th>fold</th>\n",
              "      <th>classID</th>\n",
              "      <th>class</th>\n",
              "      <th>full_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100032-3-0-0.wav</td>\n",
              "      <td>100032</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.317551</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>dog_bark</td>\n",
              "      <td>/content/drive/MyDrive/UrbanSound8K/fold5/1000...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100263-2-0-117.wav</td>\n",
              "      <td>100263</td>\n",
              "      <td>58.5</td>\n",
              "      <td>62.500000</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>children_playing</td>\n",
              "      <td>/content/drive/MyDrive/UrbanSound8K/fold5/1002...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100263-2-0-121.wav</td>\n",
              "      <td>100263</td>\n",
              "      <td>60.5</td>\n",
              "      <td>64.500000</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>children_playing</td>\n",
              "      <td>/content/drive/MyDrive/UrbanSound8K/fold5/1002...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100263-2-0-126.wav</td>\n",
              "      <td>100263</td>\n",
              "      <td>63.0</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>children_playing</td>\n",
              "      <td>/content/drive/MyDrive/UrbanSound8K/fold5/1002...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100263-2-0-137.wav</td>\n",
              "      <td>100263</td>\n",
              "      <td>68.5</td>\n",
              "      <td>72.500000</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>children_playing</td>\n",
              "      <td>/content/drive/MyDrive/UrbanSound8K/fold5/1002...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b8fec5c5-5da2-4bd1-86c5-93fe19e92f97')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b8fec5c5-5da2-4bd1-86c5-93fe19e92f97 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b8fec5c5-5da2-4bd1-86c5-93fe19e92f97');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-147791a6-a184-4a77-9d38-77bca2c0e176\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-147791a6-a184-4a77-9d38-77bca2c0e176')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-147791a6-a184-4a77-9d38-77bca2c0e176 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "audio_df",
              "summary": "{\n  \"name\": \"audio_df\",\n  \"rows\": 8732,\n  \"fields\": [\n    {\n      \"column\": \"slice_file_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8732,\n        \"samples\": [\n          \"54898-8-0-2.wav\",\n          \"172338-9-0-7.wav\",\n          \"95562-4-3-0.wav\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fsID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57991,\n        \"min\": 344,\n        \"max\": 209992,\n        \"num_unique_values\": 1297,\n        \"samples\": [\n          180257,\n          157940,\n          20015\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 74.29212630755421,\n        \"min\": 0.0,\n        \"max\": 600.125356,\n        \"num_unique_values\": 4878,\n        \"samples\": [\n          10.038318,\n          5.711988,\n          4.634753\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"end\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 74.36966935075833,\n        \"min\": 0.105962,\n        \"max\": 604.125356,\n        \"num_unique_values\": 5020,\n        \"samples\": [\n          13.168347,\n          2.653802,\n          10.286916\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"salience\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fold\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 10,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          3,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"classID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 9,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          7,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"jackhammer\",\n          \"children_playing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"full_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8732,\n        \"samples\": [\n          \"/content/drive/MyDrive/UrbanSound8K/fold3/54898-8-0-2.wav\",\n          \"/content/drive/MyDrive/UrbanSound8K/fold4/172338-9-0-7.wav\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we will split our dataset into training (70%), validation (15%), and test (15%) sets.\n",
        "\n"
      ],
      "metadata": {
        "id": "2oAs_9nuBLqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, temp_df = train_test_split(audio_df, test_size=0.3, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "J9hJxT2LBPIx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating a PyTorch Dataset\n",
        "The next step involves creating a PyTorch dataset. However, before that, we will create Hugging Face datasets using our audio files and labels.\n",
        "\n",
        "This process converts the audio files into numeric arrays that you can pass to Hugging Face transformer models. You can create a Hugging Face dataset using the datasets.Dataset.from_dict() method. Pass the full path of the audio files to the audio key and cast the audios to a sampling rate of 16khz, the default sampling rate for the Hugging Face Whisper model. In addition, we will create a key for our target labels as well.\n",
        "\n",
        "The following script creates Hugging Face datasets for train, test, and validation splits in our dataset."
      ],
      "metadata": {
        "id": "fWc-v1U0By-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_audio_dataset = datasets.Dataset.from_dict({\"audio\": train_df[\"full_path\"].tolist(),\n",
        "                                                  \"labels\": train_df[\"classID\"].tolist()    }\n",
        "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "\n",
        "test_audio_dataset = datasets.Dataset.from_dict({\"audio\": test_df[\"full_path\"].tolist(),\n",
        "                                                  \"labels\": test_df[\"classID\"].tolist() }\n",
        "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "\n",
        "val_audio_dataset = datasets.Dataset.from_dict({\"audio\": val_df[\"full_path\"].tolist(),\n",
        "                                                  \"labels\": val_df[\"classID\"].tolist()  }\n",
        "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))"
      ],
      "metadata": {
        "id": "HAkF0SZKB2rU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use the dataset above for training with the Hugging Face trainer. However, we need to create a PyTorch dataset since we want to fine-tune the Whisper model in PyTorch.\n",
        "\n",
        "You must extract audio features from your audio files to train a Whisper model. The WhisperFeatureExtractor object accomplishes this job.\n",
        "\n",
        "The script below creates a Hugging Face Whisper model object and a WhisperFeatureExtractor object from the openai/whisper-base checkpoint. You can choose any other Whisper model checkpoint from Hugging Face if desired.\n",
        "\n",
        "The code is configured to utilize the GPU if CUDA is available. Otherwise, it defaults to running on the CPU."
      ],
      "metadata": {
        "id": "8VxOBzklCBQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model_checkpoint = \"openai/whisper-base\" ## multilingual, 74M parameters\n",
        "model_checkpoint = \"openai/whisper-small\" ## multilingual, 224M parameters 11.5GB GPU RAM (fits in T4)\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_checkpoint)\n",
        "encoder = WhisperModel.from_pretrained(model_checkpoint)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "872c730394c74258a2c6407a547b6785",
            "241602e9c8b346b396558223e49e3175",
            "11fd1e0c700745a39bdc15fd53a362f5",
            "f67c9f8a87c14acaa69f52c77b575581",
            "08a7d0dd91c6454bb97d107a5366a4c5",
            "e1b144dbb54d413485b35dce6589d32f",
            "c6ca4cb24a7e4e0a9e38d814c7c3410c",
            "429e27a3d03e444b87619577dccead13",
            "6986abae03e74d4aa6f9f18a32c98ce2",
            "d97966d454994f36bd9f2432644900cf",
            "325191e8d69b4d56b09f7ff99186664c",
            "a770243d88a0479e8561078034a089ff",
            "93aa175a7cb245c1bdb982a739a7a347",
            "c358f66f72a844c7bd7eb6ce030df385",
            "5954543e14ab4970b9be69ef1734b45f",
            "ae4d9eaaba7f4a8fa6b774edcc3cb234",
            "697e0bb66de34124936ecd825d6f8ff5",
            "2f788e116c164aeca3d432f5e4830818",
            "c1ebd7414a0a4d5eb5c5a490be9a0b58",
            "36b0e1a65b1a42ffb07e60646f636f5c",
            "af1abfad565a473faf6ef428add84328",
            "083e7649258042c8ae69ef31303db34a",
            "e514c530132246ce9adc9637fbdefba5",
            "3d78b5a823c94c3b9be653f763577c07",
            "d48c8bc5db2042f08760785bcbb87990",
            "d3fe243dd59d4ff5bbf08e55754e75c8",
            "c370a026816e4708bda53ac34267c184",
            "f456651751314476837dd8346f13f523",
            "f8996ca365d34e0cbe32c866bb39db79",
            "1327dab11d0948f6aba24bd8506b4917",
            "51844eb832b64d0a8020c80d91ff11eb",
            "59f2cbb0f9044a5fbe224cc93e015273",
            "6f2ce970ff324b35851f41ee2355cfae"
          ]
        },
        "id": "v2K4Yl7CCCjw",
        "outputId": "a759ced7-8179-4cb8-9fb6-a11b12cd9d9c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "872c730394c74258a2c6407a547b6785"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.97k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a770243d88a0479e8561078034a089ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e514c530132246ce9adc9637fbdefba5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create a PyTorch dataset for our problem, will define the SpeechClassificationDataset class that inherits the torch.utils.data.Dataset class. The SpeechClassificationDataset class returns the input features and decoder inputs for the Whisper model, along with the target labels."
      ],
      "metadata": {
        "id": "XsnqqvILCSrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpeechClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, audio_data,  text_processor):\n",
        "        self.audio_data = audio_data\n",
        "        self.text_processor = text_processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "      inputs = self.text_processor(self.audio_data[index][\"audio\"][\"array\"],\n",
        "                                   return_tensors=\"pt\",\n",
        "                                   sampling_rate=self.audio_data[index][\"audio\"][\"sampling_rate\"])\n",
        "      input_features = inputs.input_features\n",
        "      ##print('size of input features',input_features.size())\n",
        "      decoder_input_ids = torch.tensor([[1, 1]]) * encoder.config.decoder_start_token_id\n",
        "\n",
        "      labels = np.array(self.audio_data[index]['labels'])\n",
        "\n",
        "      return input_features, decoder_input_ids, torch.tensor(labels)\n"
      ],
      "metadata": {
        "id": "lNuBaWCCChBG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the below script, we can transform our train_audio_dataset, test_audio_dataset, and val_audio_dataset Hugging Face datasets into PyTorch datasets. To process the datasets in batches, we create corresponding DataLoader objects with a batch size of 8."
      ],
      "metadata": {
        "id": "7gJCfFO0CmLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SpeechClassificationDataset(train_audio_dataset,  feature_extractor)\n",
        "test_dataset = SpeechClassificationDataset(test_audio_dataset,  feature_extractor)\n",
        "val_dataset = SpeechClassificationDataset(val_audio_dataset,  feature_extractor)\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "AuFitUVECoH-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fine Tuning Hugging Face Whisper Model\n",
        "We are now prepared to fine-tune the Hugging Face Whisper model on our PyTorch dataset. To achieve this, we will design a model class that takes the Whisper model encoder as a parameter and passes the encoder's output through five dense layers (4096, 2048, 1024, and 512 neurons). The final dense layer will have ten labels, reflecting our ten target classes."
      ],
      "metadata": {
        "id": "dlEX7k3qGfLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SpeechClassifier(nn.Module):\n",
        "    def __init__(self, num_labels, encoder):\n",
        "        super(SpeechClassifier, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        print(\"encoder output size\",self.encoder.config.hidden_size)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.encoder.config.hidden_size, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4096, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_features, decoder_input_ids):\n",
        "        outputs = self.encoder(input_features, decoder_input_ids=decoder_input_ids)\n",
        "        pooled_output = outputs['last_hidden_state'][:, 0, :]\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n"
      ],
      "metadata": {
        "id": "OJtLhnabGh4I"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following script, we will instantiate an object of the SpeechClassifier class, specifying the number of labels (10 in our case), along with the loss function and optimizer."
      ],
      "metadata": {
        "id": "22GWl-cEGmiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels = 10\n",
        "\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "P4XErBiyGpE6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training the Model\n",
        "To train the model, we define the train() method and pass it the model, train and validation data loaders, optimizer, criterion (loss function), device, and the number of epochs as parameters. The train() method executes the training loop and prints the loss for each batch.\n",
        "\n"
      ],
      "metadata": {
        "id": "ElWozM3_GtRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training function\n",
        "def train(model, train_loader, val_loader, optimizer,  criterion, device, num_epochs):\n",
        "\n",
        "    best_accuracy = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "\n",
        "            input_features, decoder_input_ids, labels = batch\n",
        "\n",
        "            input_features = input_features.squeeze()\n",
        "            input_features = input_features.to(device)\n",
        "\n",
        "            decoder_input_ids = decoder_input_ids.squeeze()\n",
        "            decoder_input_ids = decoder_input_ids.to(device)\n",
        "\n",
        "            labels = labels.view(-1)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits = model(input_features, decoder_input_ids)\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i+1) % 8 == 0:\n",
        "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item() :.4f}')\n",
        "                train_loss = 0.0\n",
        "\n",
        "        val_loss, val_accuracy, val_f1, _ , _ = evaluate(model, val_loader, device)\n",
        "\n",
        "        if val_accuracy > best_accuracy:\n",
        "            best_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "\n",
        "        print(\"========================================================================================\")\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}, Best Accuracy: {best_accuracy:.4f}')\n",
        "        print(\"========================================================================================\")\n",
        "\n"
      ],
      "metadata": {
        "id": "X3e1qLBNGwkP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After every epoch, the train() method displays the loss, accuracy, and F-1 score on the validation set using the evaluate() method, defined in the subsequent script. Finally, the train() method saves the model with the highest accuracy on the validation set."
      ],
      "metadata": {
        "id": "ss2Q8yOjG16v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader,  device):\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, batch in enumerate(data_loader):\n",
        "\n",
        "          input_features, decoder_input_ids, labels = batch\n",
        "\n",
        "          input_features = input_features.squeeze()\n",
        "          input_features = input_features.to(device)\n",
        "\n",
        "          decoder_input_ids = decoder_input_ids.squeeze()\n",
        "          decoder_input_ids = decoder_input_ids.to(device)\n",
        "\n",
        "          labels = labels.view(-1)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          logits = model(input_features, decoder_input_ids)\n",
        "\n",
        "          loss = criterion(logits, labels)\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          _, preds = torch.max(logits, 1)\n",
        "          all_labels.append(labels.cpu().numpy())\n",
        "          all_preds.append(preds.cpu().numpy())\n",
        "\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "\n",
        "    loss = total_loss / len(data_loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return loss, accuracy, f1, all_labels, all_preds\n"
      ],
      "metadata": {
        "id": "nhyrldcNG2wK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can train the model by invoking the train() method. In the following script, I trained the model for 5 epochs. Feel free to adjust the number of epochs based on your requirements."
      ],
      "metadata": {
        "id": "92kZn4K8G6xT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)\n",
        "#Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ln6xHfUMG7uI",
        "outputId": "bd839165-398c-4683-ca5c-f3ffb3ea6b1f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Batch 8/764, Train Loss: 2.2991\n",
            "Epoch 1/20, Batch 16/764, Train Loss: 2.0803\n",
            "Epoch 1/20, Batch 24/764, Train Loss: 1.6318\n",
            "Epoch 1/20, Batch 32/764, Train Loss: 1.7138\n",
            "Epoch 1/20, Batch 40/764, Train Loss: 1.6000\n",
            "Epoch 1/20, Batch 48/764, Train Loss: 1.0849\n",
            "Epoch 1/20, Batch 56/764, Train Loss: 0.7041\n",
            "Epoch 1/20, Batch 64/764, Train Loss: 1.7553\n",
            "Epoch 1/20, Batch 72/764, Train Loss: 0.9718\n",
            "Epoch 1/20, Batch 80/764, Train Loss: 1.1186\n",
            "Epoch 1/20, Batch 88/764, Train Loss: 1.1333\n",
            "Epoch 1/20, Batch 96/764, Train Loss: 1.0307\n",
            "Epoch 1/20, Batch 104/764, Train Loss: 0.6273\n",
            "Epoch 1/20, Batch 112/764, Train Loss: 0.9793\n",
            "Epoch 1/20, Batch 120/764, Train Loss: 1.1432\n",
            "Epoch 1/20, Batch 128/764, Train Loss: 0.3128\n",
            "Epoch 1/20, Batch 136/764, Train Loss: 0.5699\n",
            "Epoch 1/20, Batch 144/764, Train Loss: 0.7948\n",
            "Epoch 1/20, Batch 152/764, Train Loss: 1.2119\n",
            "Epoch 1/20, Batch 160/764, Train Loss: 0.6273\n",
            "Epoch 1/20, Batch 168/764, Train Loss: 0.7709\n",
            "Epoch 1/20, Batch 176/764, Train Loss: 0.7509\n",
            "Epoch 1/20, Batch 184/764, Train Loss: 1.1878\n",
            "Epoch 1/20, Batch 192/764, Train Loss: 0.8718\n",
            "Epoch 1/20, Batch 200/764, Train Loss: 1.2503\n",
            "Epoch 1/20, Batch 208/764, Train Loss: 1.5071\n",
            "Epoch 1/20, Batch 216/764, Train Loss: 0.4250\n",
            "Epoch 1/20, Batch 224/764, Train Loss: 0.6545\n",
            "Epoch 1/20, Batch 232/764, Train Loss: 0.4755\n",
            "Epoch 1/20, Batch 240/764, Train Loss: 0.5432\n",
            "Epoch 1/20, Batch 248/764, Train Loss: 0.5720\n",
            "Epoch 1/20, Batch 256/764, Train Loss: 0.6492\n",
            "Epoch 1/20, Batch 264/764, Train Loss: 1.1131\n",
            "Epoch 1/20, Batch 272/764, Train Loss: 0.7808\n",
            "Epoch 1/20, Batch 280/764, Train Loss: 0.5225\n",
            "Epoch 1/20, Batch 288/764, Train Loss: 1.5847\n",
            "Epoch 1/20, Batch 296/764, Train Loss: 1.5857\n",
            "Epoch 1/20, Batch 304/764, Train Loss: 0.4583\n",
            "Epoch 1/20, Batch 312/764, Train Loss: 0.5735\n",
            "Epoch 1/20, Batch 320/764, Train Loss: 0.1576\n",
            "Epoch 1/20, Batch 328/764, Train Loss: 0.6301\n",
            "Epoch 1/20, Batch 336/764, Train Loss: 0.7129\n",
            "Epoch 1/20, Batch 344/764, Train Loss: 0.3687\n",
            "Epoch 1/20, Batch 352/764, Train Loss: 0.3759\n",
            "Epoch 1/20, Batch 360/764, Train Loss: 0.7643\n",
            "Epoch 1/20, Batch 368/764, Train Loss: 1.0390\n",
            "Epoch 1/20, Batch 376/764, Train Loss: 0.6734\n",
            "Epoch 1/20, Batch 384/764, Train Loss: 0.5249\n",
            "Epoch 1/20, Batch 392/764, Train Loss: 1.7898\n",
            "Epoch 1/20, Batch 400/764, Train Loss: 0.4099\n",
            "Epoch 1/20, Batch 408/764, Train Loss: 0.7256\n",
            "Epoch 1/20, Batch 416/764, Train Loss: 0.4546\n",
            "Epoch 1/20, Batch 424/764, Train Loss: 0.2282\n",
            "Epoch 1/20, Batch 432/764, Train Loss: 0.1688\n",
            "Epoch 1/20, Batch 440/764, Train Loss: 0.2412\n",
            "Epoch 1/20, Batch 448/764, Train Loss: 0.3470\n",
            "Epoch 1/20, Batch 456/764, Train Loss: 0.6362\n",
            "Epoch 1/20, Batch 464/764, Train Loss: 0.3010\n",
            "Epoch 1/20, Batch 472/764, Train Loss: 0.8957\n",
            "Epoch 1/20, Batch 480/764, Train Loss: 0.7518\n",
            "Epoch 1/20, Batch 488/764, Train Loss: 0.6821\n",
            "Epoch 1/20, Batch 496/764, Train Loss: 0.7909\n",
            "Epoch 1/20, Batch 504/764, Train Loss: 0.8768\n",
            "Epoch 1/20, Batch 512/764, Train Loss: 0.4571\n",
            "Epoch 1/20, Batch 520/764, Train Loss: 0.7984\n",
            "Epoch 1/20, Batch 528/764, Train Loss: 1.0478\n",
            "Epoch 1/20, Batch 536/764, Train Loss: 0.2875\n",
            "Epoch 1/20, Batch 544/764, Train Loss: 0.5049\n",
            "Epoch 1/20, Batch 552/764, Train Loss: 0.6974\n",
            "Epoch 1/20, Batch 560/764, Train Loss: 0.8725\n",
            "Epoch 1/20, Batch 568/764, Train Loss: 0.7085\n",
            "Epoch 1/20, Batch 576/764, Train Loss: 0.0813\n",
            "Epoch 1/20, Batch 584/764, Train Loss: 0.2441\n",
            "Epoch 1/20, Batch 592/764, Train Loss: 0.0852\n",
            "Epoch 1/20, Batch 600/764, Train Loss: 0.4498\n",
            "Epoch 1/20, Batch 608/764, Train Loss: 2.3110\n",
            "Epoch 1/20, Batch 616/764, Train Loss: 0.1054\n",
            "Epoch 1/20, Batch 624/764, Train Loss: 0.4073\n",
            "Epoch 1/20, Batch 632/764, Train Loss: 1.3353\n",
            "Epoch 1/20, Batch 640/764, Train Loss: 1.0638\n",
            "Epoch 1/20, Batch 648/764, Train Loss: 0.0585\n",
            "Epoch 1/20, Batch 656/764, Train Loss: 0.2085\n",
            "Epoch 1/20, Batch 664/764, Train Loss: 1.0926\n",
            "Epoch 1/20, Batch 672/764, Train Loss: 0.1882\n",
            "Epoch 1/20, Batch 680/764, Train Loss: 0.0826\n",
            "Epoch 1/20, Batch 688/764, Train Loss: 0.4503\n",
            "Epoch 1/20, Batch 696/764, Train Loss: 0.1024\n",
            "Epoch 1/20, Batch 704/764, Train Loss: 0.0832\n",
            "Epoch 1/20, Batch 712/764, Train Loss: 1.7329\n",
            "Epoch 1/20, Batch 720/764, Train Loss: 1.2729\n",
            "Epoch 1/20, Batch 728/764, Train Loss: 0.3986\n",
            "Epoch 1/20, Batch 736/764, Train Loss: 0.9519\n",
            "Epoch 1/20, Batch 744/764, Train Loss: 0.3805\n",
            "Epoch 1/20, Batch 752/764, Train Loss: 0.7209\n",
            "Epoch 1/20, Batch 760/764, Train Loss: 0.5633\n",
            "========================================================================================\n",
            "Epoch 1/20, Val Loss: 0.4509, Val Accuracy: 0.8565, Val F1: 0.8631, Best Accuracy: 0.8565\n",
            "========================================================================================\n",
            "Epoch 2/20, Batch 8/764, Train Loss: 0.1303\n",
            "Epoch 2/20, Batch 16/764, Train Loss: 1.1142\n",
            "Epoch 2/20, Batch 24/764, Train Loss: 0.9709\n",
            "Epoch 2/20, Batch 32/764, Train Loss: 0.0759\n",
            "Epoch 2/20, Batch 40/764, Train Loss: 0.6189\n",
            "Epoch 2/20, Batch 48/764, Train Loss: 0.4567\n",
            "Epoch 2/20, Batch 56/764, Train Loss: 0.0483\n",
            "Epoch 2/20, Batch 64/764, Train Loss: 0.8658\n",
            "Epoch 2/20, Batch 72/764, Train Loss: 0.3885\n",
            "Epoch 2/20, Batch 80/764, Train Loss: 0.2244\n",
            "Epoch 2/20, Batch 88/764, Train Loss: 0.3382\n",
            "Epoch 2/20, Batch 96/764, Train Loss: 0.0372\n",
            "Epoch 2/20, Batch 104/764, Train Loss: 0.0578\n",
            "Epoch 2/20, Batch 112/764, Train Loss: 0.1896\n",
            "Epoch 2/20, Batch 120/764, Train Loss: 0.3867\n",
            "Epoch 2/20, Batch 128/764, Train Loss: 0.0256\n",
            "Epoch 2/20, Batch 136/764, Train Loss: 0.6873\n",
            "Epoch 2/20, Batch 144/764, Train Loss: 0.3036\n",
            "Epoch 2/20, Batch 152/764, Train Loss: 0.3235\n",
            "Epoch 2/20, Batch 160/764, Train Loss: 0.1253\n",
            "Epoch 2/20, Batch 168/764, Train Loss: 0.9045\n",
            "Epoch 2/20, Batch 176/764, Train Loss: 0.0586\n",
            "Epoch 2/20, Batch 184/764, Train Loss: 0.1001\n",
            "Epoch 2/20, Batch 192/764, Train Loss: 0.0218\n",
            "Epoch 2/20, Batch 200/764, Train Loss: 0.0309\n",
            "Epoch 2/20, Batch 208/764, Train Loss: 0.5210\n",
            "Epoch 2/20, Batch 216/764, Train Loss: 0.1507\n",
            "Epoch 2/20, Batch 224/764, Train Loss: 0.3459\n",
            "Epoch 2/20, Batch 232/764, Train Loss: 0.2491\n",
            "Epoch 2/20, Batch 240/764, Train Loss: 0.1813\n",
            "Epoch 2/20, Batch 248/764, Train Loss: 0.4562\n",
            "Epoch 2/20, Batch 256/764, Train Loss: 0.2860\n",
            "Epoch 2/20, Batch 264/764, Train Loss: 0.3812\n",
            "Epoch 2/20, Batch 272/764, Train Loss: 0.7261\n",
            "Epoch 2/20, Batch 280/764, Train Loss: 0.9127\n",
            "Epoch 2/20, Batch 288/764, Train Loss: 0.9440\n",
            "Epoch 2/20, Batch 296/764, Train Loss: 0.7815\n",
            "Epoch 2/20, Batch 304/764, Train Loss: 0.6770\n",
            "Epoch 2/20, Batch 312/764, Train Loss: 0.1365\n",
            "Epoch 2/20, Batch 320/764, Train Loss: 0.6433\n",
            "Epoch 2/20, Batch 328/764, Train Loss: 0.0418\n",
            "Epoch 2/20, Batch 336/764, Train Loss: 0.2381\n",
            "Epoch 2/20, Batch 344/764, Train Loss: 1.2135\n",
            "Epoch 2/20, Batch 352/764, Train Loss: 0.0868\n",
            "Epoch 2/20, Batch 360/764, Train Loss: 0.0703\n",
            "Epoch 2/20, Batch 368/764, Train Loss: 0.0412\n",
            "Epoch 2/20, Batch 376/764, Train Loss: 1.2762\n",
            "Epoch 2/20, Batch 384/764, Train Loss: 0.4829\n",
            "Epoch 2/20, Batch 392/764, Train Loss: 0.9374\n",
            "Epoch 2/20, Batch 400/764, Train Loss: 0.3894\n",
            "Epoch 2/20, Batch 408/764, Train Loss: 0.2577\n",
            "Epoch 2/20, Batch 416/764, Train Loss: 0.8281\n",
            "Epoch 2/20, Batch 424/764, Train Loss: 0.0527\n",
            "Epoch 2/20, Batch 432/764, Train Loss: 1.4342\n",
            "Epoch 2/20, Batch 440/764, Train Loss: 0.4522\n",
            "Epoch 2/20, Batch 448/764, Train Loss: 1.0609\n",
            "Epoch 2/20, Batch 456/764, Train Loss: 0.5810\n",
            "Epoch 2/20, Batch 464/764, Train Loss: 0.5492\n",
            "Epoch 2/20, Batch 472/764, Train Loss: 0.1657\n",
            "Epoch 2/20, Batch 480/764, Train Loss: 0.0094\n",
            "Epoch 2/20, Batch 488/764, Train Loss: 0.1588\n",
            "Epoch 2/20, Batch 496/764, Train Loss: 0.0872\n",
            "Epoch 2/20, Batch 504/764, Train Loss: 0.3078\n",
            "Epoch 2/20, Batch 512/764, Train Loss: 0.2055\n",
            "Epoch 2/20, Batch 520/764, Train Loss: 0.2239\n",
            "Epoch 2/20, Batch 528/764, Train Loss: 0.2030\n",
            "Epoch 2/20, Batch 536/764, Train Loss: 0.3352\n",
            "Epoch 2/20, Batch 544/764, Train Loss: 0.2427\n",
            "Epoch 2/20, Batch 552/764, Train Loss: 0.1601\n",
            "Epoch 2/20, Batch 560/764, Train Loss: 0.6463\n",
            "Epoch 2/20, Batch 568/764, Train Loss: 0.5370\n",
            "Epoch 2/20, Batch 576/764, Train Loss: 0.5062\n",
            "Epoch 2/20, Batch 584/764, Train Loss: 0.8254\n",
            "Epoch 2/20, Batch 592/764, Train Loss: 0.0871\n",
            "Epoch 2/20, Batch 600/764, Train Loss: 0.0911\n",
            "Epoch 2/20, Batch 608/764, Train Loss: 2.0392\n",
            "Epoch 2/20, Batch 616/764, Train Loss: 0.4033\n",
            "Epoch 2/20, Batch 624/764, Train Loss: 0.4328\n",
            "Epoch 2/20, Batch 632/764, Train Loss: 0.5288\n",
            "Epoch 2/20, Batch 640/764, Train Loss: 0.2055\n",
            "Epoch 2/20, Batch 648/764, Train Loss: 0.6690\n",
            "Epoch 2/20, Batch 656/764, Train Loss: 0.2523\n",
            "Epoch 2/20, Batch 664/764, Train Loss: 0.2936\n",
            "Epoch 2/20, Batch 672/764, Train Loss: 0.4234\n",
            "Epoch 2/20, Batch 680/764, Train Loss: 0.6268\n",
            "Epoch 2/20, Batch 688/764, Train Loss: 0.4380\n",
            "Epoch 2/20, Batch 696/764, Train Loss: 0.3784\n",
            "Epoch 2/20, Batch 704/764, Train Loss: 0.0491\n",
            "Epoch 2/20, Batch 712/764, Train Loss: 0.0059\n",
            "Epoch 2/20, Batch 720/764, Train Loss: 0.3062\n",
            "Epoch 2/20, Batch 728/764, Train Loss: 0.9442\n",
            "Epoch 2/20, Batch 736/764, Train Loss: 0.4405\n",
            "Epoch 2/20, Batch 744/764, Train Loss: 0.2507\n",
            "Epoch 2/20, Batch 752/764, Train Loss: 0.2586\n",
            "Epoch 2/20, Batch 760/764, Train Loss: 0.0432\n",
            "========================================================================================\n",
            "Epoch 2/20, Val Loss: 0.4294, Val Accuracy: 0.8687, Val F1: 0.8792, Best Accuracy: 0.8687\n",
            "========================================================================================\n",
            "Epoch 3/20, Batch 8/764, Train Loss: 0.1456\n",
            "Epoch 3/20, Batch 16/764, Train Loss: 0.0172\n",
            "Epoch 3/20, Batch 24/764, Train Loss: 0.0195\n",
            "Epoch 3/20, Batch 32/764, Train Loss: 0.0879\n",
            "Epoch 3/20, Batch 40/764, Train Loss: 0.0915\n",
            "Epoch 3/20, Batch 48/764, Train Loss: 0.0181\n",
            "Epoch 3/20, Batch 56/764, Train Loss: 0.5230\n",
            "Epoch 3/20, Batch 64/764, Train Loss: 0.2268\n",
            "Epoch 3/20, Batch 72/764, Train Loss: 0.0131\n",
            "Epoch 3/20, Batch 80/764, Train Loss: 1.1067\n",
            "Epoch 3/20, Batch 88/764, Train Loss: 0.6143\n",
            "Epoch 3/20, Batch 96/764, Train Loss: 0.2211\n",
            "Epoch 3/20, Batch 104/764, Train Loss: 0.1831\n",
            "Epoch 3/20, Batch 112/764, Train Loss: 0.1234\n",
            "Epoch 3/20, Batch 120/764, Train Loss: 0.5471\n",
            "Epoch 3/20, Batch 128/764, Train Loss: 0.0205\n",
            "Epoch 3/20, Batch 136/764, Train Loss: 0.1937\n",
            "Epoch 3/20, Batch 144/764, Train Loss: 0.1958\n",
            "Epoch 3/20, Batch 152/764, Train Loss: 0.0146\n",
            "Epoch 3/20, Batch 160/764, Train Loss: 1.1086\n",
            "Epoch 3/20, Batch 168/764, Train Loss: 0.1696\n",
            "Epoch 3/20, Batch 176/764, Train Loss: 0.0117\n",
            "Epoch 3/20, Batch 184/764, Train Loss: 0.0170\n",
            "Epoch 3/20, Batch 192/764, Train Loss: 0.1157\n",
            "Epoch 3/20, Batch 200/764, Train Loss: 0.3283\n",
            "Epoch 3/20, Batch 208/764, Train Loss: 0.0086\n",
            "Epoch 3/20, Batch 216/764, Train Loss: 0.0045\n",
            "Epoch 3/20, Batch 224/764, Train Loss: 0.2645\n",
            "Epoch 3/20, Batch 232/764, Train Loss: 0.0660\n",
            "Epoch 3/20, Batch 240/764, Train Loss: 0.1731\n",
            "Epoch 3/20, Batch 248/764, Train Loss: 0.0768\n",
            "Epoch 3/20, Batch 256/764, Train Loss: 0.0635\n",
            "Epoch 3/20, Batch 264/764, Train Loss: 0.0930\n",
            "Epoch 3/20, Batch 272/764, Train Loss: 0.4588\n",
            "Epoch 3/20, Batch 280/764, Train Loss: 0.0962\n",
            "Epoch 3/20, Batch 288/764, Train Loss: 0.3172\n",
            "Epoch 3/20, Batch 296/764, Train Loss: 0.0113\n",
            "Epoch 3/20, Batch 304/764, Train Loss: 0.1670\n",
            "Epoch 3/20, Batch 312/764, Train Loss: 0.3411\n",
            "Epoch 3/20, Batch 320/764, Train Loss: 1.2545\n",
            "Epoch 3/20, Batch 328/764, Train Loss: 0.9962\n",
            "Epoch 3/20, Batch 336/764, Train Loss: 0.1579\n",
            "Epoch 3/20, Batch 344/764, Train Loss: 0.0900\n",
            "Epoch 3/20, Batch 352/764, Train Loss: 0.0341\n",
            "Epoch 3/20, Batch 360/764, Train Loss: 0.4268\n",
            "Epoch 3/20, Batch 368/764, Train Loss: 0.0071\n",
            "Epoch 3/20, Batch 376/764, Train Loss: 0.0611\n",
            "Epoch 3/20, Batch 384/764, Train Loss: 0.0194\n",
            "Epoch 3/20, Batch 392/764, Train Loss: 0.0394\n",
            "Epoch 3/20, Batch 400/764, Train Loss: 0.8877\n",
            "Epoch 3/20, Batch 408/764, Train Loss: 0.3665\n",
            "Epoch 3/20, Batch 416/764, Train Loss: 0.0515\n",
            "Epoch 3/20, Batch 424/764, Train Loss: 0.0246\n",
            "Epoch 3/20, Batch 432/764, Train Loss: 0.3973\n",
            "Epoch 3/20, Batch 440/764, Train Loss: 0.0662\n",
            "Epoch 3/20, Batch 448/764, Train Loss: 0.8296\n",
            "Epoch 3/20, Batch 456/764, Train Loss: 0.2974\n",
            "Epoch 3/20, Batch 464/764, Train Loss: 0.0076\n",
            "Epoch 3/20, Batch 472/764, Train Loss: 0.0162\n",
            "Epoch 3/20, Batch 480/764, Train Loss: 0.9118\n",
            "Epoch 3/20, Batch 488/764, Train Loss: 0.8725\n",
            "Epoch 3/20, Batch 496/764, Train Loss: 0.2518\n",
            "Epoch 3/20, Batch 504/764, Train Loss: 0.4962\n",
            "Epoch 3/20, Batch 512/764, Train Loss: 0.2544\n",
            "Epoch 3/20, Batch 520/764, Train Loss: 0.0594\n",
            "Epoch 3/20, Batch 528/764, Train Loss: 0.4677\n",
            "Epoch 3/20, Batch 536/764, Train Loss: 0.1875\n",
            "Epoch 3/20, Batch 544/764, Train Loss: 0.2160\n",
            "Epoch 3/20, Batch 552/764, Train Loss: 0.1079\n",
            "Epoch 3/20, Batch 560/764, Train Loss: 0.0547\n",
            "Epoch 3/20, Batch 568/764, Train Loss: 0.0148\n",
            "Epoch 3/20, Batch 576/764, Train Loss: 0.1625\n",
            "Epoch 3/20, Batch 584/764, Train Loss: 0.0605\n",
            "Epoch 3/20, Batch 592/764, Train Loss: 0.4872\n",
            "Epoch 3/20, Batch 600/764, Train Loss: 0.0167\n",
            "Epoch 3/20, Batch 608/764, Train Loss: 0.0037\n",
            "Epoch 3/20, Batch 616/764, Train Loss: 0.5053\n",
            "Epoch 3/20, Batch 624/764, Train Loss: 0.0877\n",
            "Epoch 3/20, Batch 632/764, Train Loss: 0.1752\n",
            "Epoch 3/20, Batch 640/764, Train Loss: 0.0466\n",
            "Epoch 3/20, Batch 648/764, Train Loss: 0.4473\n",
            "Epoch 3/20, Batch 656/764, Train Loss: 0.0098\n",
            "Epoch 3/20, Batch 664/764, Train Loss: 0.3760\n",
            "Epoch 3/20, Batch 672/764, Train Loss: 0.2500\n",
            "Epoch 3/20, Batch 680/764, Train Loss: 0.4571\n",
            "Epoch 3/20, Batch 688/764, Train Loss: 0.0297\n",
            "Epoch 3/20, Batch 696/764, Train Loss: 0.4284\n",
            "Epoch 3/20, Batch 704/764, Train Loss: 0.9169\n",
            "Epoch 3/20, Batch 712/764, Train Loss: 0.5729\n",
            "Epoch 3/20, Batch 720/764, Train Loss: 1.3377\n",
            "Epoch 3/20, Batch 728/764, Train Loss: 0.3832\n",
            "Epoch 3/20, Batch 736/764, Train Loss: 0.2430\n",
            "Epoch 3/20, Batch 744/764, Train Loss: 0.4669\n",
            "Epoch 3/20, Batch 752/764, Train Loss: 0.1140\n",
            "Epoch 3/20, Batch 760/764, Train Loss: 0.6860\n",
            "========================================================================================\n",
            "Epoch 3/20, Val Loss: 0.2381, Val Accuracy: 0.9237, Val F1: 0.9321, Best Accuracy: 0.9237\n",
            "========================================================================================\n",
            "Epoch 4/20, Batch 8/764, Train Loss: 0.3476\n",
            "Epoch 4/20, Batch 16/764, Train Loss: 0.0096\n",
            "Epoch 4/20, Batch 24/764, Train Loss: 0.0242\n",
            "Epoch 4/20, Batch 32/764, Train Loss: 0.0524\n",
            "Epoch 4/20, Batch 40/764, Train Loss: 0.3955\n",
            "Epoch 4/20, Batch 48/764, Train Loss: 0.1086\n",
            "Epoch 4/20, Batch 56/764, Train Loss: 0.1073\n",
            "Epoch 4/20, Batch 64/764, Train Loss: 0.1258\n",
            "Epoch 4/20, Batch 72/764, Train Loss: 0.0639\n",
            "Epoch 4/20, Batch 80/764, Train Loss: 0.0097\n",
            "Epoch 4/20, Batch 88/764, Train Loss: 0.1046\n",
            "Epoch 4/20, Batch 96/764, Train Loss: 0.0301\n",
            "Epoch 4/20, Batch 104/764, Train Loss: 0.2206\n",
            "Epoch 4/20, Batch 112/764, Train Loss: 0.0230\n",
            "Epoch 4/20, Batch 120/764, Train Loss: 0.2139\n",
            "Epoch 4/20, Batch 128/764, Train Loss: 0.4954\n",
            "Epoch 4/20, Batch 136/764, Train Loss: 0.0808\n",
            "Epoch 4/20, Batch 144/764, Train Loss: 0.7749\n",
            "Epoch 4/20, Batch 152/764, Train Loss: 0.0383\n",
            "Epoch 4/20, Batch 160/764, Train Loss: 0.1181\n",
            "Epoch 4/20, Batch 168/764, Train Loss: 0.0236\n",
            "Epoch 4/20, Batch 176/764, Train Loss: 0.6677\n",
            "Epoch 4/20, Batch 184/764, Train Loss: 0.0121\n",
            "Epoch 4/20, Batch 192/764, Train Loss: 0.0176\n",
            "Epoch 4/20, Batch 200/764, Train Loss: 0.0274\n",
            "Epoch 4/20, Batch 208/764, Train Loss: 0.4007\n",
            "Epoch 4/20, Batch 216/764, Train Loss: 0.0211\n",
            "Epoch 4/20, Batch 224/764, Train Loss: 1.5172\n",
            "Epoch 4/20, Batch 232/764, Train Loss: 0.1090\n",
            "Epoch 4/20, Batch 240/764, Train Loss: 0.1347\n",
            "Epoch 4/20, Batch 248/764, Train Loss: 0.1572\n",
            "Epoch 4/20, Batch 256/764, Train Loss: 0.0774\n",
            "Epoch 4/20, Batch 264/764, Train Loss: 0.0181\n",
            "Epoch 4/20, Batch 272/764, Train Loss: 0.0720\n",
            "Epoch 4/20, Batch 280/764, Train Loss: 0.0824\n",
            "Epoch 4/20, Batch 288/764, Train Loss: 0.1007\n",
            "Epoch 4/20, Batch 296/764, Train Loss: 0.0155\n",
            "Epoch 4/20, Batch 304/764, Train Loss: 0.0051\n",
            "Epoch 4/20, Batch 312/764, Train Loss: 0.1115\n",
            "Epoch 4/20, Batch 320/764, Train Loss: 0.1435\n",
            "Epoch 4/20, Batch 328/764, Train Loss: 0.0176\n",
            "Epoch 4/20, Batch 336/764, Train Loss: 0.4300\n",
            "Epoch 4/20, Batch 344/764, Train Loss: 0.0046\n",
            "Epoch 4/20, Batch 352/764, Train Loss: 0.0038\n",
            "Epoch 4/20, Batch 360/764, Train Loss: 0.0154\n",
            "Epoch 4/20, Batch 368/764, Train Loss: 0.2576\n",
            "Epoch 4/20, Batch 376/764, Train Loss: 0.0076\n",
            "Epoch 4/20, Batch 384/764, Train Loss: 0.6695\n",
            "Epoch 4/20, Batch 392/764, Train Loss: 0.3546\n",
            "Epoch 4/20, Batch 400/764, Train Loss: 0.5737\n",
            "Epoch 4/20, Batch 408/764, Train Loss: 0.4428\n",
            "Epoch 4/20, Batch 416/764, Train Loss: 0.0754\n",
            "Epoch 4/20, Batch 424/764, Train Loss: 0.0201\n",
            "Epoch 4/20, Batch 432/764, Train Loss: 0.5226\n",
            "Epoch 4/20, Batch 440/764, Train Loss: 0.4873\n",
            "Epoch 4/20, Batch 448/764, Train Loss: 0.0204\n",
            "Epoch 4/20, Batch 456/764, Train Loss: 0.1253\n",
            "Epoch 4/20, Batch 464/764, Train Loss: 0.9225\n",
            "Epoch 4/20, Batch 472/764, Train Loss: 1.0877\n",
            "Epoch 4/20, Batch 480/764, Train Loss: 0.0183\n",
            "Epoch 4/20, Batch 488/764, Train Loss: 0.0246\n",
            "Epoch 4/20, Batch 496/764, Train Loss: 0.0307\n",
            "Epoch 4/20, Batch 504/764, Train Loss: 0.0091\n",
            "Epoch 4/20, Batch 512/764, Train Loss: 0.0896\n",
            "Epoch 4/20, Batch 520/764, Train Loss: 0.0759\n",
            "Epoch 4/20, Batch 528/764, Train Loss: 0.0052\n",
            "Epoch 4/20, Batch 536/764, Train Loss: 0.5623\n",
            "Epoch 4/20, Batch 544/764, Train Loss: 0.0600\n",
            "Epoch 4/20, Batch 552/764, Train Loss: 0.0198\n",
            "Epoch 4/20, Batch 560/764, Train Loss: 0.6725\n",
            "Epoch 4/20, Batch 568/764, Train Loss: 0.3028\n",
            "Epoch 4/20, Batch 576/764, Train Loss: 0.0151\n",
            "Epoch 4/20, Batch 584/764, Train Loss: 0.0090\n",
            "Epoch 4/20, Batch 592/764, Train Loss: 0.0426\n",
            "Epoch 4/20, Batch 600/764, Train Loss: 0.3648\n",
            "Epoch 4/20, Batch 608/764, Train Loss: 0.0687\n",
            "Epoch 4/20, Batch 616/764, Train Loss: 0.0182\n",
            "Epoch 4/20, Batch 624/764, Train Loss: 0.0348\n",
            "Epoch 4/20, Batch 632/764, Train Loss: 0.2248\n",
            "Epoch 4/20, Batch 640/764, Train Loss: 0.1680\n",
            "Epoch 4/20, Batch 648/764, Train Loss: 0.0034\n",
            "Epoch 4/20, Batch 656/764, Train Loss: 0.4772\n",
            "Epoch 4/20, Batch 664/764, Train Loss: 0.0348\n",
            "Epoch 4/20, Batch 672/764, Train Loss: 0.0571\n",
            "Epoch 4/20, Batch 680/764, Train Loss: 0.2235\n",
            "Epoch 4/20, Batch 688/764, Train Loss: 0.0142\n",
            "Epoch 4/20, Batch 696/764, Train Loss: 0.7504\n",
            "Epoch 4/20, Batch 704/764, Train Loss: 0.0153\n",
            "Epoch 4/20, Batch 712/764, Train Loss: 0.1650\n",
            "Epoch 4/20, Batch 720/764, Train Loss: 0.2519\n",
            "Epoch 4/20, Batch 728/764, Train Loss: 0.3082\n",
            "Epoch 4/20, Batch 736/764, Train Loss: 0.0031\n",
            "Epoch 4/20, Batch 744/764, Train Loss: 0.3527\n",
            "Epoch 4/20, Batch 752/764, Train Loss: 0.4965\n",
            "Epoch 4/20, Batch 760/764, Train Loss: 0.0277\n",
            "========================================================================================\n",
            "Epoch 4/20, Val Loss: 0.3178, Val Accuracy: 0.9153, Val F1: 0.9245, Best Accuracy: 0.9237\n",
            "========================================================================================\n",
            "Epoch 5/20, Batch 8/764, Train Loss: 0.0375\n",
            "Epoch 5/20, Batch 16/764, Train Loss: 0.0039\n",
            "Epoch 5/20, Batch 24/764, Train Loss: 0.1383\n",
            "Epoch 5/20, Batch 32/764, Train Loss: 0.1997\n",
            "Epoch 5/20, Batch 40/764, Train Loss: 0.0041\n",
            "Epoch 5/20, Batch 48/764, Train Loss: 0.0067\n",
            "Epoch 5/20, Batch 56/764, Train Loss: 0.0812\n",
            "Epoch 5/20, Batch 64/764, Train Loss: 0.5645\n",
            "Epoch 5/20, Batch 72/764, Train Loss: 0.2571\n",
            "Epoch 5/20, Batch 80/764, Train Loss: 0.0076\n",
            "Epoch 5/20, Batch 88/764, Train Loss: 0.0036\n",
            "Epoch 5/20, Batch 96/764, Train Loss: 0.4508\n",
            "Epoch 5/20, Batch 104/764, Train Loss: 0.1772\n",
            "Epoch 5/20, Batch 112/764, Train Loss: 0.0256\n",
            "Epoch 5/20, Batch 120/764, Train Loss: 0.0576\n",
            "Epoch 5/20, Batch 128/764, Train Loss: 0.0052\n",
            "Epoch 5/20, Batch 136/764, Train Loss: 0.3637\n",
            "Epoch 5/20, Batch 144/764, Train Loss: 0.1634\n",
            "Epoch 5/20, Batch 152/764, Train Loss: 0.0128\n",
            "Epoch 5/20, Batch 160/764, Train Loss: 0.0255\n",
            "Epoch 5/20, Batch 168/764, Train Loss: 0.0032\n",
            "Epoch 5/20, Batch 176/764, Train Loss: 0.5403\n",
            "Epoch 5/20, Batch 184/764, Train Loss: 0.0095\n",
            "Epoch 5/20, Batch 192/764, Train Loss: 0.0275\n",
            "Epoch 5/20, Batch 200/764, Train Loss: 0.0720\n",
            "Epoch 5/20, Batch 208/764, Train Loss: 0.5416\n",
            "Epoch 5/20, Batch 216/764, Train Loss: 0.0683\n",
            "Epoch 5/20, Batch 224/764, Train Loss: 0.0603\n",
            "Epoch 5/20, Batch 232/764, Train Loss: 0.0210\n",
            "Epoch 5/20, Batch 240/764, Train Loss: 0.0029\n",
            "Epoch 5/20, Batch 248/764, Train Loss: 0.8173\n",
            "Epoch 5/20, Batch 256/764, Train Loss: 0.0013\n",
            "Epoch 5/20, Batch 264/764, Train Loss: 0.0116\n",
            "Epoch 5/20, Batch 272/764, Train Loss: 0.0114\n",
            "Epoch 5/20, Batch 280/764, Train Loss: 0.0083\n",
            "Epoch 5/20, Batch 288/764, Train Loss: 0.0883\n",
            "Epoch 5/20, Batch 296/764, Train Loss: 0.3638\n",
            "Epoch 5/20, Batch 304/764, Train Loss: 0.6160\n",
            "Epoch 5/20, Batch 312/764, Train Loss: 0.1471\n",
            "Epoch 5/20, Batch 320/764, Train Loss: 0.0512\n",
            "Epoch 5/20, Batch 328/764, Train Loss: 0.7802\n",
            "Epoch 5/20, Batch 336/764, Train Loss: 0.1665\n",
            "Epoch 5/20, Batch 344/764, Train Loss: 0.0041\n",
            "Epoch 5/20, Batch 352/764, Train Loss: 0.0653\n",
            "Epoch 5/20, Batch 360/764, Train Loss: 0.0400\n",
            "Epoch 5/20, Batch 368/764, Train Loss: 0.0289\n",
            "Epoch 5/20, Batch 376/764, Train Loss: 0.0110\n",
            "Epoch 5/20, Batch 384/764, Train Loss: 0.0053\n",
            "Epoch 5/20, Batch 392/764, Train Loss: 0.1993\n",
            "Epoch 5/20, Batch 400/764, Train Loss: 0.0081\n",
            "Epoch 5/20, Batch 408/764, Train Loss: 0.2788\n",
            "Epoch 5/20, Batch 416/764, Train Loss: 0.0693\n",
            "Epoch 5/20, Batch 424/764, Train Loss: 0.1206\n",
            "Epoch 5/20, Batch 432/764, Train Loss: 0.1394\n",
            "Epoch 5/20, Batch 440/764, Train Loss: 0.0154\n",
            "Epoch 5/20, Batch 448/764, Train Loss: 0.2020\n",
            "Epoch 5/20, Batch 456/764, Train Loss: 0.0859\n",
            "Epoch 5/20, Batch 464/764, Train Loss: 0.0018\n",
            "Epoch 5/20, Batch 472/764, Train Loss: 0.0097\n",
            "Epoch 5/20, Batch 480/764, Train Loss: 0.0290\n",
            "Epoch 5/20, Batch 488/764, Train Loss: 0.0604\n",
            "Epoch 5/20, Batch 496/764, Train Loss: 0.6575\n",
            "Epoch 5/20, Batch 504/764, Train Loss: 0.0792\n",
            "Epoch 5/20, Batch 512/764, Train Loss: 0.0117\n",
            "Epoch 5/20, Batch 520/764, Train Loss: 0.0156\n",
            "Epoch 5/20, Batch 528/764, Train Loss: 0.0433\n",
            "Epoch 5/20, Batch 536/764, Train Loss: 0.0137\n",
            "Epoch 5/20, Batch 544/764, Train Loss: 0.2852\n",
            "Epoch 5/20, Batch 552/764, Train Loss: 0.1220\n",
            "Epoch 5/20, Batch 560/764, Train Loss: 0.0167\n",
            "Epoch 5/20, Batch 568/764, Train Loss: 0.0209\n",
            "Epoch 5/20, Batch 576/764, Train Loss: 0.2926\n",
            "Epoch 5/20, Batch 584/764, Train Loss: 0.0662\n",
            "Epoch 5/20, Batch 592/764, Train Loss: 0.0144\n",
            "Epoch 5/20, Batch 600/764, Train Loss: 0.0006\n",
            "Epoch 5/20, Batch 608/764, Train Loss: 0.0029\n",
            "Epoch 5/20, Batch 616/764, Train Loss: 0.0041\n",
            "Epoch 5/20, Batch 624/764, Train Loss: 0.0840\n",
            "Epoch 5/20, Batch 632/764, Train Loss: 0.0148\n",
            "Epoch 5/20, Batch 640/764, Train Loss: 0.0011\n",
            "Epoch 5/20, Batch 648/764, Train Loss: 0.0040\n",
            "Epoch 5/20, Batch 656/764, Train Loss: 0.1320\n",
            "Epoch 5/20, Batch 664/764, Train Loss: 0.1109\n",
            "Epoch 5/20, Batch 672/764, Train Loss: 0.7264\n",
            "Epoch 5/20, Batch 680/764, Train Loss: 0.0840\n",
            "Epoch 5/20, Batch 688/764, Train Loss: 0.0166\n",
            "Epoch 5/20, Batch 696/764, Train Loss: 0.0035\n",
            "Epoch 5/20, Batch 704/764, Train Loss: 0.0811\n",
            "Epoch 5/20, Batch 712/764, Train Loss: 0.0712\n",
            "Epoch 5/20, Batch 720/764, Train Loss: 0.0080\n",
            "Epoch 5/20, Batch 728/764, Train Loss: 0.0023\n",
            "Epoch 5/20, Batch 736/764, Train Loss: 0.0021\n",
            "Epoch 5/20, Batch 744/764, Train Loss: 0.0026\n",
            "Epoch 5/20, Batch 752/764, Train Loss: 0.2923\n",
            "Epoch 5/20, Batch 760/764, Train Loss: 0.0387\n",
            "========================================================================================\n",
            "Epoch 5/20, Val Loss: 0.2696, Val Accuracy: 0.9252, Val F1: 0.9340, Best Accuracy: 0.9252\n",
            "========================================================================================\n",
            "Epoch 6/20, Batch 8/764, Train Loss: 0.0046\n",
            "Epoch 6/20, Batch 16/764, Train Loss: 0.0030\n",
            "Epoch 6/20, Batch 24/764, Train Loss: 0.0654\n",
            "Epoch 6/20, Batch 32/764, Train Loss: 0.0023\n",
            "Epoch 6/20, Batch 40/764, Train Loss: 0.0129\n",
            "Epoch 6/20, Batch 48/764, Train Loss: 0.0102\n",
            "Epoch 6/20, Batch 56/764, Train Loss: 0.0017\n",
            "Epoch 6/20, Batch 64/764, Train Loss: 0.0905\n",
            "Epoch 6/20, Batch 72/764, Train Loss: 0.2606\n",
            "Epoch 6/20, Batch 80/764, Train Loss: 0.5218\n",
            "Epoch 6/20, Batch 88/764, Train Loss: 0.0178\n",
            "Epoch 6/20, Batch 96/764, Train Loss: 0.0009\n",
            "Epoch 6/20, Batch 104/764, Train Loss: 0.0158\n",
            "Epoch 6/20, Batch 112/764, Train Loss: 0.0624\n",
            "Epoch 6/20, Batch 120/764, Train Loss: 0.0145\n",
            "Epoch 6/20, Batch 128/764, Train Loss: 0.3400\n",
            "Epoch 6/20, Batch 136/764, Train Loss: 0.4162\n",
            "Epoch 6/20, Batch 144/764, Train Loss: 0.0374\n",
            "Epoch 6/20, Batch 152/764, Train Loss: 0.0556\n",
            "Epoch 6/20, Batch 160/764, Train Loss: 0.0025\n",
            "Epoch 6/20, Batch 168/764, Train Loss: 0.0222\n",
            "Epoch 6/20, Batch 176/764, Train Loss: 0.0043\n",
            "Epoch 6/20, Batch 184/764, Train Loss: 0.0282\n",
            "Epoch 6/20, Batch 192/764, Train Loss: 0.1545\n",
            "Epoch 6/20, Batch 200/764, Train Loss: 0.0027\n",
            "Epoch 6/20, Batch 208/764, Train Loss: 0.0135\n",
            "Epoch 6/20, Batch 216/764, Train Loss: 0.1017\n",
            "Epoch 6/20, Batch 224/764, Train Loss: 0.0109\n",
            "Epoch 6/20, Batch 232/764, Train Loss: 0.0523\n",
            "Epoch 6/20, Batch 240/764, Train Loss: 0.0074\n",
            "Epoch 6/20, Batch 248/764, Train Loss: 0.0187\n",
            "Epoch 6/20, Batch 256/764, Train Loss: 0.0490\n",
            "Epoch 6/20, Batch 264/764, Train Loss: 0.0064\n",
            "Epoch 6/20, Batch 272/764, Train Loss: 0.1094\n",
            "Epoch 6/20, Batch 280/764, Train Loss: 0.0037\n",
            "Epoch 6/20, Batch 288/764, Train Loss: 0.0030\n",
            "Epoch 6/20, Batch 296/764, Train Loss: 0.0021\n",
            "Epoch 6/20, Batch 304/764, Train Loss: 0.0288\n",
            "Epoch 6/20, Batch 312/764, Train Loss: 0.4682\n",
            "Epoch 6/20, Batch 320/764, Train Loss: 0.0855\n",
            "Epoch 6/20, Batch 328/764, Train Loss: 0.0169\n",
            "Epoch 6/20, Batch 336/764, Train Loss: 0.3013\n",
            "Epoch 6/20, Batch 344/764, Train Loss: 0.0061\n",
            "Epoch 6/20, Batch 352/764, Train Loss: 0.0062\n",
            "Epoch 6/20, Batch 360/764, Train Loss: 0.5434\n",
            "Epoch 6/20, Batch 368/764, Train Loss: 0.5931\n",
            "Epoch 6/20, Batch 376/764, Train Loss: 0.1857\n",
            "Epoch 6/20, Batch 384/764, Train Loss: 0.0059\n",
            "Epoch 6/20, Batch 392/764, Train Loss: 0.0234\n",
            "Epoch 6/20, Batch 400/764, Train Loss: 0.3076\n",
            "Epoch 6/20, Batch 408/764, Train Loss: 0.0428\n",
            "Epoch 6/20, Batch 416/764, Train Loss: 0.3091\n",
            "Epoch 6/20, Batch 424/764, Train Loss: 0.0440\n",
            "Epoch 6/20, Batch 432/764, Train Loss: 0.0245\n",
            "Epoch 6/20, Batch 440/764, Train Loss: 0.0210\n",
            "Epoch 6/20, Batch 448/764, Train Loss: 0.1359\n",
            "Epoch 6/20, Batch 456/764, Train Loss: 0.0345\n",
            "Epoch 6/20, Batch 464/764, Train Loss: 0.0153\n",
            "Epoch 6/20, Batch 472/764, Train Loss: 0.2465\n",
            "Epoch 6/20, Batch 480/764, Train Loss: 0.0222\n",
            "Epoch 6/20, Batch 488/764, Train Loss: 0.4369\n",
            "Epoch 6/20, Batch 496/764, Train Loss: 0.4493\n",
            "Epoch 6/20, Batch 504/764, Train Loss: 0.0857\n",
            "Epoch 6/20, Batch 512/764, Train Loss: 0.0125\n",
            "Epoch 6/20, Batch 520/764, Train Loss: 0.1367\n",
            "Epoch 6/20, Batch 528/764, Train Loss: 0.0193\n",
            "Epoch 6/20, Batch 536/764, Train Loss: 0.0021\n",
            "Epoch 6/20, Batch 544/764, Train Loss: 0.2091\n",
            "Epoch 6/20, Batch 552/764, Train Loss: 0.0215\n",
            "Epoch 6/20, Batch 560/764, Train Loss: 0.0678\n",
            "Epoch 6/20, Batch 568/764, Train Loss: 0.1541\n",
            "Epoch 6/20, Batch 576/764, Train Loss: 0.0820\n",
            "Epoch 6/20, Batch 584/764, Train Loss: 0.0297\n",
            "Epoch 6/20, Batch 592/764, Train Loss: 0.0925\n",
            "Epoch 6/20, Batch 600/764, Train Loss: 0.0258\n",
            "Epoch 6/20, Batch 608/764, Train Loss: 0.0582\n",
            "Epoch 6/20, Batch 616/764, Train Loss: 0.0521\n",
            "Epoch 6/20, Batch 624/764, Train Loss: 0.3561\n",
            "Epoch 6/20, Batch 632/764, Train Loss: 0.0020\n",
            "Epoch 6/20, Batch 640/764, Train Loss: 0.0055\n",
            "Epoch 6/20, Batch 648/764, Train Loss: 0.0037\n",
            "Epoch 6/20, Batch 656/764, Train Loss: 0.0055\n",
            "Epoch 6/20, Batch 664/764, Train Loss: 0.0044\n",
            "Epoch 6/20, Batch 672/764, Train Loss: 0.0947\n",
            "Epoch 6/20, Batch 680/764, Train Loss: 0.2481\n",
            "Epoch 6/20, Batch 688/764, Train Loss: 0.1213\n",
            "Epoch 6/20, Batch 696/764, Train Loss: 0.3204\n",
            "Epoch 6/20, Batch 704/764, Train Loss: 0.2452\n",
            "Epoch 6/20, Batch 712/764, Train Loss: 0.1503\n",
            "Epoch 6/20, Batch 720/764, Train Loss: 0.2621\n",
            "Epoch 6/20, Batch 728/764, Train Loss: 0.0115\n",
            "Epoch 6/20, Batch 736/764, Train Loss: 0.0176\n",
            "Epoch 6/20, Batch 744/764, Train Loss: 0.1494\n",
            "Epoch 6/20, Batch 752/764, Train Loss: 0.0089\n",
            "Epoch 6/20, Batch 760/764, Train Loss: 0.0123\n",
            "========================================================================================\n",
            "Epoch 6/20, Val Loss: 0.2869, Val Accuracy: 0.9237, Val F1: 0.9296, Best Accuracy: 0.9252\n",
            "========================================================================================\n",
            "Epoch 7/20, Batch 8/764, Train Loss: 0.0053\n",
            "Epoch 7/20, Batch 16/764, Train Loss: 0.0269\n",
            "Epoch 7/20, Batch 24/764, Train Loss: 0.0009\n",
            "Epoch 7/20, Batch 32/764, Train Loss: 0.0028\n",
            "Epoch 7/20, Batch 40/764, Train Loss: 0.0049\n",
            "Epoch 7/20, Batch 48/764, Train Loss: 0.0054\n",
            "Epoch 7/20, Batch 56/764, Train Loss: 0.0339\n",
            "Epoch 7/20, Batch 64/764, Train Loss: 0.0085\n",
            "Epoch 7/20, Batch 72/764, Train Loss: 0.0678\n",
            "Epoch 7/20, Batch 80/764, Train Loss: 0.1674\n",
            "Epoch 7/20, Batch 88/764, Train Loss: 0.0010\n",
            "Epoch 7/20, Batch 96/764, Train Loss: 0.5234\n",
            "Epoch 7/20, Batch 104/764, Train Loss: 1.0858\n",
            "Epoch 7/20, Batch 112/764, Train Loss: 0.0760\n",
            "Epoch 7/20, Batch 120/764, Train Loss: 0.7258\n",
            "Epoch 7/20, Batch 128/764, Train Loss: 0.4816\n",
            "Epoch 7/20, Batch 136/764, Train Loss: 0.0064\n",
            "Epoch 7/20, Batch 144/764, Train Loss: 0.0019\n",
            "Epoch 7/20, Batch 152/764, Train Loss: 0.0073\n",
            "Epoch 7/20, Batch 160/764, Train Loss: 0.0098\n",
            "Epoch 7/20, Batch 168/764, Train Loss: 0.0023\n",
            "Epoch 7/20, Batch 176/764, Train Loss: 0.0238\n",
            "Epoch 7/20, Batch 184/764, Train Loss: 0.0054\n",
            "Epoch 7/20, Batch 192/764, Train Loss: 0.0025\n",
            "Epoch 7/20, Batch 200/764, Train Loss: 0.0088\n",
            "Epoch 7/20, Batch 208/764, Train Loss: 0.0014\n",
            "Epoch 7/20, Batch 216/764, Train Loss: 0.0917\n",
            "Epoch 7/20, Batch 224/764, Train Loss: 0.0021\n",
            "Epoch 7/20, Batch 232/764, Train Loss: 0.0407\n",
            "Epoch 7/20, Batch 240/764, Train Loss: 0.0560\n",
            "Epoch 7/20, Batch 248/764, Train Loss: 0.2375\n",
            "Epoch 7/20, Batch 256/764, Train Loss: 0.0287\n",
            "Epoch 7/20, Batch 264/764, Train Loss: 0.0103\n",
            "Epoch 7/20, Batch 272/764, Train Loss: 0.0104\n",
            "Epoch 7/20, Batch 280/764, Train Loss: 0.0212\n",
            "Epoch 7/20, Batch 288/764, Train Loss: 0.0028\n",
            "Epoch 7/20, Batch 296/764, Train Loss: 0.0120\n",
            "Epoch 7/20, Batch 304/764, Train Loss: 0.0011\n",
            "Epoch 7/20, Batch 312/764, Train Loss: 0.0274\n",
            "Epoch 7/20, Batch 320/764, Train Loss: 0.0042\n",
            "Epoch 7/20, Batch 328/764, Train Loss: 0.0641\n",
            "Epoch 7/20, Batch 336/764, Train Loss: 0.0035\n",
            "Epoch 7/20, Batch 344/764, Train Loss: 0.3328\n",
            "Epoch 7/20, Batch 352/764, Train Loss: 0.0006\n",
            "Epoch 7/20, Batch 360/764, Train Loss: 0.0011\n",
            "Epoch 7/20, Batch 368/764, Train Loss: 0.0386\n",
            "Epoch 7/20, Batch 376/764, Train Loss: 0.0407\n",
            "Epoch 7/20, Batch 384/764, Train Loss: 0.0013\n",
            "Epoch 7/20, Batch 392/764, Train Loss: 0.0357\n",
            "Epoch 7/20, Batch 400/764, Train Loss: 0.0031\n",
            "Epoch 7/20, Batch 408/764, Train Loss: 0.1456\n",
            "Epoch 7/20, Batch 416/764, Train Loss: 0.0064\n",
            "Epoch 7/20, Batch 424/764, Train Loss: 0.0029\n",
            "Epoch 7/20, Batch 432/764, Train Loss: 0.0331\n",
            "Epoch 7/20, Batch 440/764, Train Loss: 0.3871\n",
            "Epoch 7/20, Batch 448/764, Train Loss: 0.0438\n",
            "Epoch 7/20, Batch 456/764, Train Loss: 0.0021\n",
            "Epoch 7/20, Batch 464/764, Train Loss: 0.0008\n",
            "Epoch 7/20, Batch 472/764, Train Loss: 0.0434\n",
            "Epoch 7/20, Batch 480/764, Train Loss: 0.0013\n",
            "Epoch 7/20, Batch 488/764, Train Loss: 0.3327\n",
            "Epoch 7/20, Batch 496/764, Train Loss: 0.0056\n",
            "Epoch 7/20, Batch 504/764, Train Loss: 0.7241\n",
            "Epoch 7/20, Batch 512/764, Train Loss: 0.0035\n",
            "Epoch 7/20, Batch 520/764, Train Loss: 0.0110\n",
            "Epoch 7/20, Batch 528/764, Train Loss: 0.0174\n",
            "Epoch 7/20, Batch 536/764, Train Loss: 0.2486\n",
            "Epoch 7/20, Batch 544/764, Train Loss: 0.1186\n",
            "Epoch 7/20, Batch 552/764, Train Loss: 0.0194\n",
            "Epoch 7/20, Batch 560/764, Train Loss: 0.0332\n",
            "Epoch 7/20, Batch 568/764, Train Loss: 0.1597\n",
            "Epoch 7/20, Batch 576/764, Train Loss: 0.0010\n",
            "Epoch 7/20, Batch 584/764, Train Loss: 0.1800\n",
            "Epoch 7/20, Batch 592/764, Train Loss: 0.0018\n",
            "Epoch 7/20, Batch 600/764, Train Loss: 0.1720\n",
            "Epoch 7/20, Batch 608/764, Train Loss: 0.2027\n",
            "Epoch 7/20, Batch 616/764, Train Loss: 0.0031\n",
            "Epoch 7/20, Batch 624/764, Train Loss: 0.0006\n",
            "Epoch 7/20, Batch 632/764, Train Loss: 0.0048\n",
            "Epoch 7/20, Batch 640/764, Train Loss: 0.1214\n",
            "Epoch 7/20, Batch 648/764, Train Loss: 0.0040\n",
            "Epoch 7/20, Batch 656/764, Train Loss: 0.0006\n",
            "Epoch 7/20, Batch 664/764, Train Loss: 0.4949\n",
            "Epoch 7/20, Batch 672/764, Train Loss: 0.1571\n",
            "Epoch 7/20, Batch 680/764, Train Loss: 0.0021\n",
            "Epoch 7/20, Batch 688/764, Train Loss: 0.5803\n",
            "Epoch 7/20, Batch 696/764, Train Loss: 0.0722\n",
            "Epoch 7/20, Batch 704/764, Train Loss: 0.8451\n",
            "Epoch 7/20, Batch 712/764, Train Loss: 0.3584\n",
            "Epoch 7/20, Batch 720/764, Train Loss: 0.0159\n",
            "Epoch 7/20, Batch 728/764, Train Loss: 0.0204\n",
            "Epoch 7/20, Batch 736/764, Train Loss: 0.0125\n",
            "Epoch 7/20, Batch 744/764, Train Loss: 0.0556\n",
            "Epoch 7/20, Batch 752/764, Train Loss: 0.0616\n",
            "Epoch 7/20, Batch 760/764, Train Loss: 0.0143\n",
            "========================================================================================\n",
            "Epoch 7/20, Val Loss: 0.2839, Val Accuracy: 0.9290, Val F1: 0.9354, Best Accuracy: 0.9290\n",
            "========================================================================================\n",
            "Epoch 8/20, Batch 8/764, Train Loss: 0.0023\n",
            "Epoch 8/20, Batch 16/764, Train Loss: 0.0003\n",
            "Epoch 8/20, Batch 24/764, Train Loss: 0.0008\n",
            "Epoch 8/20, Batch 32/764, Train Loss: 0.0014\n",
            "Epoch 8/20, Batch 40/764, Train Loss: 0.0296\n",
            "Epoch 8/20, Batch 48/764, Train Loss: 0.1348\n",
            "Epoch 8/20, Batch 56/764, Train Loss: 0.0005\n",
            "Epoch 8/20, Batch 64/764, Train Loss: 0.0139\n",
            "Epoch 8/20, Batch 72/764, Train Loss: 0.0025\n",
            "Epoch 8/20, Batch 80/764, Train Loss: 0.1383\n",
            "Epoch 8/20, Batch 88/764, Train Loss: 0.5874\n",
            "Epoch 8/20, Batch 96/764, Train Loss: 0.0618\n",
            "Epoch 8/20, Batch 104/764, Train Loss: 0.0008\n",
            "Epoch 8/20, Batch 112/764, Train Loss: 0.1746\n",
            "Epoch 8/20, Batch 120/764, Train Loss: 0.0255\n",
            "Epoch 8/20, Batch 128/764, Train Loss: 0.1469\n",
            "Epoch 8/20, Batch 136/764, Train Loss: 0.0323\n",
            "Epoch 8/20, Batch 144/764, Train Loss: 0.3285\n",
            "Epoch 8/20, Batch 152/764, Train Loss: 0.3817\n",
            "Epoch 8/20, Batch 160/764, Train Loss: 1.2297\n",
            "Epoch 8/20, Batch 168/764, Train Loss: 0.0624\n",
            "Epoch 8/20, Batch 176/764, Train Loss: 0.0221\n",
            "Epoch 8/20, Batch 184/764, Train Loss: 0.1306\n",
            "Epoch 8/20, Batch 192/764, Train Loss: 0.0054\n",
            "Epoch 8/20, Batch 200/764, Train Loss: 0.0082\n",
            "Epoch 8/20, Batch 208/764, Train Loss: 0.0057\n",
            "Epoch 8/20, Batch 216/764, Train Loss: 0.0015\n",
            "Epoch 8/20, Batch 224/764, Train Loss: 0.0062\n",
            "Epoch 8/20, Batch 232/764, Train Loss: 0.0021\n",
            "Epoch 8/20, Batch 240/764, Train Loss: 0.0068\n",
            "Epoch 8/20, Batch 248/764, Train Loss: 0.0129\n",
            "Epoch 8/20, Batch 256/764, Train Loss: 0.0024\n",
            "Epoch 8/20, Batch 264/764, Train Loss: 0.0015\n",
            "Epoch 8/20, Batch 272/764, Train Loss: 0.0037\n",
            "Epoch 8/20, Batch 280/764, Train Loss: 0.0358\n",
            "Epoch 8/20, Batch 288/764, Train Loss: 0.0104\n",
            "Epoch 8/20, Batch 296/764, Train Loss: 0.3863\n",
            "Epoch 8/20, Batch 304/764, Train Loss: 0.2773\n",
            "Epoch 8/20, Batch 312/764, Train Loss: 0.0019\n",
            "Epoch 8/20, Batch 320/764, Train Loss: 0.0088\n",
            "Epoch 8/20, Batch 328/764, Train Loss: 0.0061\n",
            "Epoch 8/20, Batch 336/764, Train Loss: 0.0629\n",
            "Epoch 8/20, Batch 344/764, Train Loss: 0.4180\n",
            "Epoch 8/20, Batch 352/764, Train Loss: 0.0020\n",
            "Epoch 8/20, Batch 360/764, Train Loss: 0.0291\n",
            "Epoch 8/20, Batch 368/764, Train Loss: 0.0590\n",
            "Epoch 8/20, Batch 376/764, Train Loss: 0.2526\n",
            "Epoch 8/20, Batch 384/764, Train Loss: 0.0211\n",
            "Epoch 8/20, Batch 392/764, Train Loss: 0.0272\n",
            "Epoch 8/20, Batch 400/764, Train Loss: 0.7526\n",
            "Epoch 8/20, Batch 408/764, Train Loss: 0.1248\n",
            "Epoch 8/20, Batch 416/764, Train Loss: 0.0079\n",
            "Epoch 8/20, Batch 424/764, Train Loss: 0.0077\n",
            "Epoch 8/20, Batch 432/764, Train Loss: 0.0120\n",
            "Epoch 8/20, Batch 440/764, Train Loss: 0.0664\n",
            "Epoch 8/20, Batch 448/764, Train Loss: 0.2258\n",
            "Epoch 8/20, Batch 456/764, Train Loss: 0.3518\n",
            "Epoch 8/20, Batch 464/764, Train Loss: 0.0040\n",
            "Epoch 8/20, Batch 472/764, Train Loss: 0.0073\n",
            "Epoch 8/20, Batch 480/764, Train Loss: 0.0041\n",
            "Epoch 8/20, Batch 488/764, Train Loss: 0.0127\n",
            "Epoch 8/20, Batch 496/764, Train Loss: 0.0623\n",
            "Epoch 8/20, Batch 504/764, Train Loss: 0.0635\n",
            "Epoch 8/20, Batch 512/764, Train Loss: 0.1407\n",
            "Epoch 8/20, Batch 520/764, Train Loss: 0.0009\n",
            "Epoch 8/20, Batch 528/764, Train Loss: 0.1010\n",
            "Epoch 8/20, Batch 536/764, Train Loss: 0.6885\n",
            "Epoch 8/20, Batch 544/764, Train Loss: 0.0053\n",
            "Epoch 8/20, Batch 552/764, Train Loss: 0.0098\n",
            "Epoch 8/20, Batch 560/764, Train Loss: 0.1611\n",
            "Epoch 8/20, Batch 568/764, Train Loss: 0.0395\n",
            "Epoch 8/20, Batch 576/764, Train Loss: 0.0077\n",
            "Epoch 8/20, Batch 584/764, Train Loss: 0.0136\n",
            "Epoch 8/20, Batch 592/764, Train Loss: 0.0060\n",
            "Epoch 8/20, Batch 600/764, Train Loss: 0.0201\n",
            "Epoch 8/20, Batch 608/764, Train Loss: 0.0022\n",
            "Epoch 8/20, Batch 616/764, Train Loss: 0.0037\n",
            "Epoch 8/20, Batch 624/764, Train Loss: 0.0024\n",
            "Epoch 8/20, Batch 632/764, Train Loss: 0.1328\n",
            "Epoch 8/20, Batch 640/764, Train Loss: 0.4770\n",
            "Epoch 8/20, Batch 648/764, Train Loss: 0.2063\n",
            "Epoch 8/20, Batch 656/764, Train Loss: 0.0193\n",
            "Epoch 8/20, Batch 664/764, Train Loss: 0.0030\n",
            "Epoch 8/20, Batch 672/764, Train Loss: 0.1086\n",
            "Epoch 8/20, Batch 680/764, Train Loss: 0.0071\n",
            "Epoch 8/20, Batch 688/764, Train Loss: 0.0111\n",
            "Epoch 8/20, Batch 696/764, Train Loss: 0.5080\n",
            "Epoch 8/20, Batch 704/764, Train Loss: 0.0217\n",
            "Epoch 8/20, Batch 712/764, Train Loss: 0.0688\n",
            "Epoch 8/20, Batch 720/764, Train Loss: 0.0182\n",
            "Epoch 8/20, Batch 728/764, Train Loss: 0.2395\n",
            "Epoch 8/20, Batch 736/764, Train Loss: 0.0741\n",
            "Epoch 8/20, Batch 744/764, Train Loss: 0.5520\n",
            "Epoch 8/20, Batch 752/764, Train Loss: 0.1775\n",
            "Epoch 8/20, Batch 760/764, Train Loss: 0.0071\n",
            "========================================================================================\n",
            "Epoch 8/20, Val Loss: 0.3354, Val Accuracy: 0.9183, Val F1: 0.9169, Best Accuracy: 0.9290\n",
            "========================================================================================\n",
            "Epoch 9/20, Batch 8/764, Train Loss: 0.0507\n",
            "Epoch 9/20, Batch 16/764, Train Loss: 0.0770\n",
            "Epoch 9/20, Batch 24/764, Train Loss: 0.0442\n",
            "Epoch 9/20, Batch 32/764, Train Loss: 0.0045\n",
            "Epoch 9/20, Batch 40/764, Train Loss: 0.0052\n",
            "Epoch 9/20, Batch 48/764, Train Loss: 0.0251\n",
            "Epoch 9/20, Batch 56/764, Train Loss: 0.0109\n",
            "Epoch 9/20, Batch 64/764, Train Loss: 0.0033\n",
            "Epoch 9/20, Batch 72/764, Train Loss: 0.0041\n",
            "Epoch 9/20, Batch 80/764, Train Loss: 0.0021\n",
            "Epoch 9/20, Batch 88/764, Train Loss: 0.0089\n",
            "Epoch 9/20, Batch 96/764, Train Loss: 0.0007\n",
            "Epoch 9/20, Batch 104/764, Train Loss: 0.0160\n",
            "Epoch 9/20, Batch 112/764, Train Loss: 0.0192\n",
            "Epoch 9/20, Batch 120/764, Train Loss: 0.0169\n",
            "Epoch 9/20, Batch 128/764, Train Loss: 0.1196\n",
            "Epoch 9/20, Batch 136/764, Train Loss: 0.0117\n",
            "Epoch 9/20, Batch 144/764, Train Loss: 0.0083\n",
            "Epoch 9/20, Batch 152/764, Train Loss: 0.3568\n",
            "Epoch 9/20, Batch 160/764, Train Loss: 0.0032\n",
            "Epoch 9/20, Batch 168/764, Train Loss: 0.0124\n",
            "Epoch 9/20, Batch 176/764, Train Loss: 0.0028\n",
            "Epoch 9/20, Batch 184/764, Train Loss: 0.3569\n",
            "Epoch 9/20, Batch 192/764, Train Loss: 0.3030\n",
            "Epoch 9/20, Batch 200/764, Train Loss: 0.1534\n",
            "Epoch 9/20, Batch 208/764, Train Loss: 0.4535\n",
            "Epoch 9/20, Batch 216/764, Train Loss: 0.0226\n",
            "Epoch 9/20, Batch 224/764, Train Loss: 0.0182\n",
            "Epoch 9/20, Batch 232/764, Train Loss: 0.0099\n",
            "Epoch 9/20, Batch 240/764, Train Loss: 0.0351\n",
            "Epoch 9/20, Batch 248/764, Train Loss: 0.0229\n",
            "Epoch 9/20, Batch 256/764, Train Loss: 0.0039\n",
            "Epoch 9/20, Batch 264/764, Train Loss: 0.0022\n",
            "Epoch 9/20, Batch 272/764, Train Loss: 0.0120\n",
            "Epoch 9/20, Batch 280/764, Train Loss: 0.0272\n",
            "Epoch 9/20, Batch 288/764, Train Loss: 0.1162\n",
            "Epoch 9/20, Batch 296/764, Train Loss: 0.4533\n",
            "Epoch 9/20, Batch 304/764, Train Loss: 0.0013\n",
            "Epoch 9/20, Batch 312/764, Train Loss: 0.0028\n",
            "Epoch 9/20, Batch 320/764, Train Loss: 0.0186\n",
            "Epoch 9/20, Batch 328/764, Train Loss: 0.0022\n",
            "Epoch 9/20, Batch 336/764, Train Loss: 0.0009\n",
            "Epoch 9/20, Batch 344/764, Train Loss: 0.3052\n",
            "Epoch 9/20, Batch 352/764, Train Loss: 0.0424\n",
            "Epoch 9/20, Batch 360/764, Train Loss: 0.0031\n",
            "Epoch 9/20, Batch 368/764, Train Loss: 0.0229\n",
            "Epoch 9/20, Batch 376/764, Train Loss: 0.2610\n",
            "Epoch 9/20, Batch 384/764, Train Loss: 0.0016\n",
            "Epoch 9/20, Batch 392/764, Train Loss: 0.0234\n",
            "Epoch 9/20, Batch 400/764, Train Loss: 0.0086\n",
            "Epoch 9/20, Batch 408/764, Train Loss: 0.9371\n",
            "Epoch 9/20, Batch 416/764, Train Loss: 0.0010\n",
            "Epoch 9/20, Batch 424/764, Train Loss: 0.0230\n",
            "Epoch 9/20, Batch 432/764, Train Loss: 0.0004\n",
            "Epoch 9/20, Batch 440/764, Train Loss: 0.0264\n",
            "Epoch 9/20, Batch 448/764, Train Loss: 0.0101\n",
            "Epoch 9/20, Batch 456/764, Train Loss: 0.0246\n",
            "Epoch 9/20, Batch 464/764, Train Loss: 0.0021\n",
            "Epoch 9/20, Batch 472/764, Train Loss: 0.0191\n",
            "Epoch 9/20, Batch 480/764, Train Loss: 0.0078\n",
            "Epoch 9/20, Batch 488/764, Train Loss: 0.0118\n",
            "Epoch 9/20, Batch 496/764, Train Loss: 0.0003\n",
            "Epoch 9/20, Batch 504/764, Train Loss: 0.0003\n",
            "Epoch 9/20, Batch 512/764, Train Loss: 0.0034\n",
            "Epoch 9/20, Batch 520/764, Train Loss: 0.0063\n",
            "Epoch 9/20, Batch 528/764, Train Loss: 0.1039\n",
            "Epoch 9/20, Batch 536/764, Train Loss: 0.0305\n",
            "Epoch 9/20, Batch 544/764, Train Loss: 0.2637\n",
            "Epoch 9/20, Batch 552/764, Train Loss: 0.0092\n",
            "Epoch 9/20, Batch 560/764, Train Loss: 0.0016\n",
            "Epoch 9/20, Batch 568/764, Train Loss: 0.0312\n",
            "Epoch 9/20, Batch 576/764, Train Loss: 0.0015\n",
            "Epoch 9/20, Batch 584/764, Train Loss: 0.0232\n",
            "Epoch 9/20, Batch 592/764, Train Loss: 0.1379\n",
            "Epoch 9/20, Batch 600/764, Train Loss: 0.3371\n",
            "Epoch 9/20, Batch 608/764, Train Loss: 0.0237\n",
            "Epoch 9/20, Batch 616/764, Train Loss: 0.1181\n",
            "Epoch 9/20, Batch 624/764, Train Loss: 0.9072\n",
            "Epoch 9/20, Batch 632/764, Train Loss: 0.0134\n",
            "Epoch 9/20, Batch 640/764, Train Loss: 0.0091\n",
            "Epoch 9/20, Batch 648/764, Train Loss: 0.4416\n",
            "Epoch 9/20, Batch 656/764, Train Loss: 0.2302\n",
            "Epoch 9/20, Batch 664/764, Train Loss: 0.0081\n",
            "Epoch 9/20, Batch 672/764, Train Loss: 0.4650\n",
            "Epoch 9/20, Batch 680/764, Train Loss: 0.0004\n",
            "Epoch 9/20, Batch 688/764, Train Loss: 0.0704\n",
            "Epoch 9/20, Batch 696/764, Train Loss: 0.0016\n",
            "Epoch 9/20, Batch 704/764, Train Loss: 0.1885\n",
            "Epoch 9/20, Batch 712/764, Train Loss: 0.0495\n",
            "Epoch 9/20, Batch 720/764, Train Loss: 0.6124\n",
            "Epoch 9/20, Batch 728/764, Train Loss: 0.0022\n",
            "Epoch 9/20, Batch 736/764, Train Loss: 0.1867\n",
            "Epoch 9/20, Batch 744/764, Train Loss: 0.0236\n",
            "Epoch 9/20, Batch 752/764, Train Loss: 0.0480\n",
            "Epoch 9/20, Batch 760/764, Train Loss: 0.0075\n",
            "========================================================================================\n",
            "Epoch 9/20, Val Loss: 0.3070, Val Accuracy: 0.9191, Val F1: 0.9249, Best Accuracy: 0.9290\n",
            "========================================================================================\n",
            "Epoch 10/20, Batch 8/764, Train Loss: 0.0065\n",
            "Epoch 10/20, Batch 16/764, Train Loss: 0.0052\n",
            "Epoch 10/20, Batch 24/764, Train Loss: 0.1113\n",
            "Epoch 10/20, Batch 32/764, Train Loss: 0.0071\n",
            "Epoch 10/20, Batch 40/764, Train Loss: 0.0005\n",
            "Epoch 10/20, Batch 48/764, Train Loss: 0.0004\n",
            "Epoch 10/20, Batch 56/764, Train Loss: 0.0012\n",
            "Epoch 10/20, Batch 64/764, Train Loss: 0.0875\n",
            "Epoch 10/20, Batch 72/764, Train Loss: 0.0015\n",
            "Epoch 10/20, Batch 80/764, Train Loss: 0.0020\n",
            "Epoch 10/20, Batch 88/764, Train Loss: 0.2001\n",
            "Epoch 10/20, Batch 96/764, Train Loss: 0.0259\n",
            "Epoch 10/20, Batch 104/764, Train Loss: 0.0013\n",
            "Epoch 10/20, Batch 112/764, Train Loss: 0.0772\n",
            "Epoch 10/20, Batch 120/764, Train Loss: 0.0234\n",
            "Epoch 10/20, Batch 128/764, Train Loss: 0.0035\n",
            "Epoch 10/20, Batch 136/764, Train Loss: 0.0158\n",
            "Epoch 10/20, Batch 144/764, Train Loss: 0.0041\n",
            "Epoch 10/20, Batch 152/764, Train Loss: 0.0012\n",
            "Epoch 10/20, Batch 160/764, Train Loss: 0.0060\n",
            "Epoch 10/20, Batch 168/764, Train Loss: 0.0121\n",
            "Epoch 10/20, Batch 176/764, Train Loss: 0.1418\n",
            "Epoch 10/20, Batch 184/764, Train Loss: 0.0037\n",
            "Epoch 10/20, Batch 192/764, Train Loss: 0.0163\n",
            "Epoch 10/20, Batch 200/764, Train Loss: 0.0045\n",
            "Epoch 10/20, Batch 208/764, Train Loss: 0.8525\n",
            "Epoch 10/20, Batch 216/764, Train Loss: 0.0224\n",
            "Epoch 10/20, Batch 224/764, Train Loss: 0.0217\n",
            "Epoch 10/20, Batch 232/764, Train Loss: 0.8196\n",
            "Epoch 10/20, Batch 240/764, Train Loss: 0.0040\n",
            "Epoch 10/20, Batch 248/764, Train Loss: 0.1072\n",
            "Epoch 10/20, Batch 256/764, Train Loss: 0.0061\n",
            "Epoch 10/20, Batch 264/764, Train Loss: 0.7587\n",
            "Epoch 10/20, Batch 272/764, Train Loss: 0.0085\n",
            "Epoch 10/20, Batch 280/764, Train Loss: 0.2761\n",
            "Epoch 10/20, Batch 288/764, Train Loss: 0.0132\n",
            "Epoch 10/20, Batch 296/764, Train Loss: 0.0184\n",
            "Epoch 10/20, Batch 304/764, Train Loss: 0.0027\n",
            "Epoch 10/20, Batch 312/764, Train Loss: 0.0012\n",
            "Epoch 10/20, Batch 320/764, Train Loss: 0.0019\n",
            "Epoch 10/20, Batch 328/764, Train Loss: 0.0013\n",
            "Epoch 10/20, Batch 336/764, Train Loss: 0.2244\n",
            "Epoch 10/20, Batch 344/764, Train Loss: 0.2384\n",
            "Epoch 10/20, Batch 352/764, Train Loss: 0.0025\n",
            "Epoch 10/20, Batch 360/764, Train Loss: 0.1095\n",
            "Epoch 10/20, Batch 368/764, Train Loss: 0.3829\n",
            "Epoch 10/20, Batch 376/764, Train Loss: 0.2045\n",
            "Epoch 10/20, Batch 384/764, Train Loss: 0.0035\n",
            "Epoch 10/20, Batch 392/764, Train Loss: 0.0178\n",
            "Epoch 10/20, Batch 400/764, Train Loss: 0.3289\n",
            "Epoch 10/20, Batch 408/764, Train Loss: 0.0328\n",
            "Epoch 10/20, Batch 416/764, Train Loss: 1.1980\n",
            "Epoch 10/20, Batch 424/764, Train Loss: 0.0191\n",
            "Epoch 10/20, Batch 432/764, Train Loss: 0.5236\n",
            "Epoch 10/20, Batch 440/764, Train Loss: 0.0045\n",
            "Epoch 10/20, Batch 448/764, Train Loss: 0.0144\n",
            "Epoch 10/20, Batch 456/764, Train Loss: 0.0257\n",
            "Epoch 10/20, Batch 464/764, Train Loss: 0.0378\n",
            "Epoch 10/20, Batch 472/764, Train Loss: 0.0013\n",
            "Epoch 10/20, Batch 480/764, Train Loss: 0.0111\n",
            "Epoch 10/20, Batch 488/764, Train Loss: 0.0062\n",
            "Epoch 10/20, Batch 496/764, Train Loss: 0.0137\n",
            "Epoch 10/20, Batch 504/764, Train Loss: 0.0017\n",
            "Epoch 10/20, Batch 512/764, Train Loss: 0.3338\n",
            "Epoch 10/20, Batch 520/764, Train Loss: 0.0071\n",
            "Epoch 10/20, Batch 528/764, Train Loss: 0.0110\n",
            "Epoch 10/20, Batch 536/764, Train Loss: 0.4484\n",
            "Epoch 10/20, Batch 544/764, Train Loss: 0.0073\n",
            "Epoch 10/20, Batch 552/764, Train Loss: 0.3760\n",
            "Epoch 10/20, Batch 560/764, Train Loss: 0.0229\n",
            "Epoch 10/20, Batch 568/764, Train Loss: 0.0802\n",
            "Epoch 10/20, Batch 576/764, Train Loss: 0.0079\n",
            "Epoch 10/20, Batch 584/764, Train Loss: 0.0168\n",
            "Epoch 10/20, Batch 592/764, Train Loss: 0.0013\n",
            "Epoch 10/20, Batch 600/764, Train Loss: 0.0107\n",
            "Epoch 10/20, Batch 608/764, Train Loss: 0.0026\n",
            "Epoch 10/20, Batch 616/764, Train Loss: 0.0018\n",
            "Epoch 10/20, Batch 624/764, Train Loss: 0.0514\n",
            "Epoch 10/20, Batch 632/764, Train Loss: 0.0107\n",
            "Epoch 10/20, Batch 640/764, Train Loss: 0.0011\n",
            "Epoch 10/20, Batch 648/764, Train Loss: 0.0008\n",
            "Epoch 10/20, Batch 656/764, Train Loss: 0.2566\n",
            "Epoch 10/20, Batch 664/764, Train Loss: 0.2387\n",
            "Epoch 10/20, Batch 672/764, Train Loss: 0.0004\n",
            "Epoch 10/20, Batch 680/764, Train Loss: 0.0017\n",
            "Epoch 10/20, Batch 688/764, Train Loss: 0.1339\n",
            "Epoch 10/20, Batch 696/764, Train Loss: 0.0072\n",
            "Epoch 10/20, Batch 704/764, Train Loss: 0.0119\n",
            "Epoch 10/20, Batch 712/764, Train Loss: 0.1364\n",
            "Epoch 10/20, Batch 720/764, Train Loss: 0.0079\n",
            "Epoch 10/20, Batch 728/764, Train Loss: 0.0179\n",
            "Epoch 10/20, Batch 736/764, Train Loss: 0.0008\n",
            "Epoch 10/20, Batch 744/764, Train Loss: 0.0008\n",
            "Epoch 10/20, Batch 752/764, Train Loss: 0.0067\n",
            "Epoch 10/20, Batch 760/764, Train Loss: 0.0298\n",
            "========================================================================================\n",
            "Epoch 10/20, Val Loss: 0.2137, Val Accuracy: 0.9443, Val F1: 0.9489, Best Accuracy: 0.9443\n",
            "========================================================================================\n",
            "Epoch 11/20, Batch 8/764, Train Loss: 0.0959\n",
            "Epoch 11/20, Batch 16/764, Train Loss: 0.2253\n",
            "Epoch 11/20, Batch 24/764, Train Loss: 0.0084\n",
            "Epoch 11/20, Batch 32/764, Train Loss: 0.0645\n",
            "Epoch 11/20, Batch 40/764, Train Loss: 0.0054\n",
            "Epoch 11/20, Batch 48/764, Train Loss: 0.0017\n",
            "Epoch 11/20, Batch 56/764, Train Loss: 0.0018\n",
            "Epoch 11/20, Batch 64/764, Train Loss: 0.0178\n",
            "Epoch 11/20, Batch 72/764, Train Loss: 1.1465\n",
            "Epoch 11/20, Batch 80/764, Train Loss: 0.0018\n",
            "Epoch 11/20, Batch 88/764, Train Loss: 0.0164\n",
            "Epoch 11/20, Batch 96/764, Train Loss: 0.0108\n",
            "Epoch 11/20, Batch 104/764, Train Loss: 0.0151\n",
            "Epoch 11/20, Batch 112/764, Train Loss: 0.0187\n",
            "Epoch 11/20, Batch 120/764, Train Loss: 0.0028\n",
            "Epoch 11/20, Batch 128/764, Train Loss: 0.0203\n",
            "Epoch 11/20, Batch 136/764, Train Loss: 0.0220\n",
            "Epoch 11/20, Batch 144/764, Train Loss: 0.0002\n",
            "Epoch 11/20, Batch 152/764, Train Loss: 0.0007\n",
            "Epoch 11/20, Batch 160/764, Train Loss: 0.0021\n",
            "Epoch 11/20, Batch 168/764, Train Loss: 0.0020\n",
            "Epoch 11/20, Batch 176/764, Train Loss: 1.0140\n",
            "Epoch 11/20, Batch 184/764, Train Loss: 0.0130\n",
            "Epoch 11/20, Batch 192/764, Train Loss: 0.0085\n",
            "Epoch 11/20, Batch 200/764, Train Loss: 0.0028\n",
            "Epoch 11/20, Batch 208/764, Train Loss: 0.1655\n",
            "Epoch 11/20, Batch 216/764, Train Loss: 0.0009\n",
            "Epoch 11/20, Batch 224/764, Train Loss: 0.0015\n",
            "Epoch 11/20, Batch 232/764, Train Loss: 0.0024\n",
            "Epoch 11/20, Batch 240/764, Train Loss: 0.0006\n",
            "Epoch 11/20, Batch 248/764, Train Loss: 0.0027\n",
            "Epoch 11/20, Batch 256/764, Train Loss: 0.0002\n",
            "Epoch 11/20, Batch 264/764, Train Loss: 0.0076\n",
            "Epoch 11/20, Batch 272/764, Train Loss: 0.0034\n",
            "Epoch 11/20, Batch 280/764, Train Loss: 0.0023\n",
            "Epoch 11/20, Batch 288/764, Train Loss: 0.2088\n",
            "Epoch 11/20, Batch 296/764, Train Loss: 0.0386\n",
            "Epoch 11/20, Batch 304/764, Train Loss: 0.0092\n",
            "Epoch 11/20, Batch 312/764, Train Loss: 0.0002\n",
            "Epoch 11/20, Batch 320/764, Train Loss: 0.0002\n",
            "Epoch 11/20, Batch 328/764, Train Loss: 0.0002\n",
            "Epoch 11/20, Batch 336/764, Train Loss: 0.0026\n",
            "Epoch 11/20, Batch 344/764, Train Loss: 0.0025\n",
            "Epoch 11/20, Batch 352/764, Train Loss: 0.0015\n",
            "Epoch 11/20, Batch 360/764, Train Loss: 0.2849\n",
            "Epoch 11/20, Batch 368/764, Train Loss: 0.0006\n",
            "Epoch 11/20, Batch 376/764, Train Loss: 0.4311\n",
            "Epoch 11/20, Batch 384/764, Train Loss: 0.0013\n",
            "Epoch 11/20, Batch 392/764, Train Loss: 0.0004\n",
            "Epoch 11/20, Batch 400/764, Train Loss: 0.0010\n",
            "Epoch 11/20, Batch 408/764, Train Loss: 0.0002\n",
            "Epoch 11/20, Batch 416/764, Train Loss: 0.0018\n",
            "Epoch 11/20, Batch 424/764, Train Loss: 0.0091\n",
            "Epoch 11/20, Batch 432/764, Train Loss: 0.0014\n",
            "Epoch 11/20, Batch 440/764, Train Loss: 0.0019\n",
            "Epoch 11/20, Batch 448/764, Train Loss: 0.0009\n",
            "Epoch 11/20, Batch 456/764, Train Loss: 0.0174\n",
            "Epoch 11/20, Batch 464/764, Train Loss: 0.4799\n",
            "Epoch 11/20, Batch 472/764, Train Loss: 0.2467\n",
            "Epoch 11/20, Batch 480/764, Train Loss: 0.0053\n",
            "Epoch 11/20, Batch 488/764, Train Loss: 0.0022\n",
            "Epoch 11/20, Batch 496/764, Train Loss: 0.2517\n",
            "Epoch 11/20, Batch 504/764, Train Loss: 0.0159\n",
            "Epoch 11/20, Batch 512/764, Train Loss: 0.6828\n",
            "Epoch 11/20, Batch 520/764, Train Loss: 0.0312\n",
            "Epoch 11/20, Batch 528/764, Train Loss: 0.0131\n",
            "Epoch 11/20, Batch 536/764, Train Loss: 0.1181\n",
            "Epoch 11/20, Batch 544/764, Train Loss: 0.0152\n",
            "Epoch 11/20, Batch 552/764, Train Loss: 0.1079\n",
            "Epoch 11/20, Batch 560/764, Train Loss: 0.0104\n",
            "Epoch 11/20, Batch 568/764, Train Loss: 0.0014\n",
            "Epoch 11/20, Batch 576/764, Train Loss: 0.0045\n",
            "Epoch 11/20, Batch 584/764, Train Loss: 0.0010\n",
            "Epoch 11/20, Batch 592/764, Train Loss: 0.0054\n",
            "Epoch 11/20, Batch 600/764, Train Loss: 0.0096\n",
            "Epoch 11/20, Batch 608/764, Train Loss: 0.0047\n",
            "Epoch 11/20, Batch 616/764, Train Loss: 0.0306\n",
            "Epoch 11/20, Batch 624/764, Train Loss: 0.0012\n",
            "Epoch 11/20, Batch 632/764, Train Loss: 0.0013\n",
            "Epoch 11/20, Batch 640/764, Train Loss: 0.0005\n",
            "Epoch 11/20, Batch 648/764, Train Loss: 0.0011\n",
            "Epoch 11/20, Batch 656/764, Train Loss: 0.0022\n",
            "Epoch 11/20, Batch 664/764, Train Loss: 0.0018\n",
            "Epoch 11/20, Batch 672/764, Train Loss: 0.0047\n",
            "Epoch 11/20, Batch 680/764, Train Loss: 0.0097\n",
            "Epoch 11/20, Batch 688/764, Train Loss: 0.0004\n",
            "Epoch 11/20, Batch 696/764, Train Loss: 0.0007\n",
            "Epoch 11/20, Batch 704/764, Train Loss: 0.0022\n",
            "Epoch 11/20, Batch 712/764, Train Loss: 0.0029\n",
            "Epoch 11/20, Batch 720/764, Train Loss: 0.0019\n",
            "Epoch 11/20, Batch 728/764, Train Loss: 0.0281\n",
            "Epoch 11/20, Batch 736/764, Train Loss: 0.0055\n",
            "Epoch 11/20, Batch 744/764, Train Loss: 0.0058\n",
            "Epoch 11/20, Batch 752/764, Train Loss: 0.0023\n",
            "Epoch 11/20, Batch 760/764, Train Loss: 0.0064\n",
            "========================================================================================\n",
            "Epoch 11/20, Val Loss: 0.3072, Val Accuracy: 0.9351, Val F1: 0.9391, Best Accuracy: 0.9443\n",
            "========================================================================================\n",
            "Epoch 12/20, Batch 8/764, Train Loss: 0.3114\n",
            "Epoch 12/20, Batch 16/764, Train Loss: 0.1060\n",
            "Epoch 12/20, Batch 24/764, Train Loss: 0.3890\n",
            "Epoch 12/20, Batch 32/764, Train Loss: 0.0003\n",
            "Epoch 12/20, Batch 40/764, Train Loss: 0.0537\n",
            "Epoch 12/20, Batch 48/764, Train Loss: 0.0002\n",
            "Epoch 12/20, Batch 56/764, Train Loss: 0.0007\n",
            "Epoch 12/20, Batch 64/764, Train Loss: 0.0070\n",
            "Epoch 12/20, Batch 72/764, Train Loss: 0.0217\n",
            "Epoch 12/20, Batch 80/764, Train Loss: 0.0165\n",
            "Epoch 12/20, Batch 88/764, Train Loss: 0.0002\n",
            "Epoch 12/20, Batch 96/764, Train Loss: 0.0106\n",
            "Epoch 12/20, Batch 104/764, Train Loss: 0.0004\n",
            "Epoch 12/20, Batch 112/764, Train Loss: 0.0757\n",
            "Epoch 12/20, Batch 120/764, Train Loss: 1.0677\n",
            "Epoch 12/20, Batch 128/764, Train Loss: 0.1292\n",
            "Epoch 12/20, Batch 136/764, Train Loss: 0.0346\n",
            "Epoch 12/20, Batch 144/764, Train Loss: 0.0688\n",
            "Epoch 12/20, Batch 152/764, Train Loss: 0.0098\n",
            "Epoch 12/20, Batch 160/764, Train Loss: 0.0247\n",
            "Epoch 12/20, Batch 168/764, Train Loss: 0.0006\n",
            "Epoch 12/20, Batch 176/764, Train Loss: 0.0365\n",
            "Epoch 12/20, Batch 184/764, Train Loss: 0.2331\n",
            "Epoch 12/20, Batch 192/764, Train Loss: 0.0174\n",
            "Epoch 12/20, Batch 200/764, Train Loss: 0.0304\n",
            "Epoch 12/20, Batch 208/764, Train Loss: 0.0003\n",
            "Epoch 12/20, Batch 216/764, Train Loss: 0.0012\n",
            "Epoch 12/20, Batch 224/764, Train Loss: 0.0006\n",
            "Epoch 12/20, Batch 232/764, Train Loss: 0.0020\n",
            "Epoch 12/20, Batch 240/764, Train Loss: 0.0004\n",
            "Epoch 12/20, Batch 248/764, Train Loss: 0.0003\n",
            "Epoch 12/20, Batch 256/764, Train Loss: 0.0002\n",
            "Epoch 12/20, Batch 264/764, Train Loss: 0.0122\n",
            "Epoch 12/20, Batch 272/764, Train Loss: 0.0104\n",
            "Epoch 12/20, Batch 280/764, Train Loss: 0.0199\n",
            "Epoch 12/20, Batch 288/764, Train Loss: 0.0038\n",
            "Epoch 12/20, Batch 296/764, Train Loss: 0.0767\n",
            "Epoch 12/20, Batch 304/764, Train Loss: 0.0128\n",
            "Epoch 12/20, Batch 312/764, Train Loss: 0.0005\n",
            "Epoch 12/20, Batch 320/764, Train Loss: 0.0019\n",
            "Epoch 12/20, Batch 328/764, Train Loss: 0.0004\n",
            "Epoch 12/20, Batch 336/764, Train Loss: 0.0012\n",
            "Epoch 12/20, Batch 344/764, Train Loss: 0.0027\n",
            "Epoch 12/20, Batch 352/764, Train Loss: 0.0012\n",
            "Epoch 12/20, Batch 360/764, Train Loss: 0.0012\n",
            "Epoch 12/20, Batch 368/764, Train Loss: 0.0050\n",
            "Epoch 12/20, Batch 376/764, Train Loss: 0.1317\n",
            "Epoch 12/20, Batch 384/764, Train Loss: 0.0013\n",
            "Epoch 12/20, Batch 392/764, Train Loss: 0.3278\n",
            "Epoch 12/20, Batch 400/764, Train Loss: 0.0005\n",
            "Epoch 12/20, Batch 408/764, Train Loss: 0.0019\n",
            "Epoch 12/20, Batch 416/764, Train Loss: 0.0017\n",
            "Epoch 12/20, Batch 424/764, Train Loss: 0.0043\n",
            "Epoch 12/20, Batch 432/764, Train Loss: 0.0066\n",
            "Epoch 12/20, Batch 440/764, Train Loss: 0.0003\n",
            "Epoch 12/20, Batch 448/764, Train Loss: 0.0437\n",
            "Epoch 12/20, Batch 456/764, Train Loss: 0.0008\n",
            "Epoch 12/20, Batch 464/764, Train Loss: 0.0004\n",
            "Epoch 12/20, Batch 472/764, Train Loss: 0.0082\n",
            "Epoch 12/20, Batch 480/764, Train Loss: 0.0032\n",
            "Epoch 12/20, Batch 488/764, Train Loss: 0.0118\n",
            "Epoch 12/20, Batch 496/764, Train Loss: 0.0107\n",
            "Epoch 12/20, Batch 504/764, Train Loss: 0.0013\n",
            "Epoch 12/20, Batch 512/764, Train Loss: 0.0096\n",
            "Epoch 12/20, Batch 520/764, Train Loss: 0.0136\n",
            "Epoch 12/20, Batch 528/764, Train Loss: 0.4540\n",
            "Epoch 12/20, Batch 536/764, Train Loss: 0.0041\n",
            "Epoch 12/20, Batch 544/764, Train Loss: 0.0049\n",
            "Epoch 12/20, Batch 552/764, Train Loss: 0.2091\n",
            "Epoch 12/20, Batch 560/764, Train Loss: 0.0046\n",
            "Epoch 12/20, Batch 568/764, Train Loss: 0.0058\n",
            "Epoch 12/20, Batch 576/764, Train Loss: 0.0006\n",
            "Epoch 12/20, Batch 584/764, Train Loss: 0.0011\n",
            "Epoch 12/20, Batch 592/764, Train Loss: 0.0022\n",
            "Epoch 12/20, Batch 600/764, Train Loss: 0.0023\n",
            "Epoch 12/20, Batch 608/764, Train Loss: 0.0035\n",
            "Epoch 12/20, Batch 616/764, Train Loss: 0.0739\n",
            "Epoch 12/20, Batch 624/764, Train Loss: 0.0032\n",
            "Epoch 12/20, Batch 632/764, Train Loss: 0.0005\n",
            "Epoch 12/20, Batch 640/764, Train Loss: 0.6715\n",
            "Epoch 12/20, Batch 648/764, Train Loss: 0.0019\n",
            "Epoch 12/20, Batch 656/764, Train Loss: 0.0016\n",
            "Epoch 12/20, Batch 664/764, Train Loss: 0.0326\n",
            "Epoch 12/20, Batch 672/764, Train Loss: 0.0050\n",
            "Epoch 12/20, Batch 680/764, Train Loss: 0.0041\n",
            "Epoch 12/20, Batch 688/764, Train Loss: 0.0030\n",
            "Epoch 12/20, Batch 696/764, Train Loss: 0.0078\n",
            "Epoch 12/20, Batch 704/764, Train Loss: 0.0032\n",
            "Epoch 12/20, Batch 712/764, Train Loss: 0.0028\n",
            "Epoch 12/20, Batch 720/764, Train Loss: 0.0014\n",
            "Epoch 12/20, Batch 728/764, Train Loss: 0.3074\n",
            "Epoch 12/20, Batch 736/764, Train Loss: 0.0134\n",
            "Epoch 12/20, Batch 744/764, Train Loss: 0.0106\n",
            "Epoch 12/20, Batch 752/764, Train Loss: 0.0244\n",
            "Epoch 12/20, Batch 760/764, Train Loss: 0.0051\n",
            "========================================================================================\n",
            "Epoch 12/20, Val Loss: 0.2988, Val Accuracy: 0.9214, Val F1: 0.9259, Best Accuracy: 0.9443\n",
            "========================================================================================\n",
            "Epoch 13/20, Batch 8/764, Train Loss: 0.0008\n",
            "Epoch 13/20, Batch 16/764, Train Loss: 0.0036\n",
            "Epoch 13/20, Batch 24/764, Train Loss: 0.1566\n",
            "Epoch 13/20, Batch 32/764, Train Loss: 0.0018\n",
            "Epoch 13/20, Batch 40/764, Train Loss: 0.1556\n",
            "Epoch 13/20, Batch 48/764, Train Loss: 0.0442\n",
            "Epoch 13/20, Batch 56/764, Train Loss: 0.0293\n",
            "Epoch 13/20, Batch 64/764, Train Loss: 0.0032\n",
            "Epoch 13/20, Batch 72/764, Train Loss: 0.0045\n",
            "Epoch 13/20, Batch 80/764, Train Loss: 0.0085\n",
            "Epoch 13/20, Batch 88/764, Train Loss: 0.0277\n",
            "Epoch 13/20, Batch 96/764, Train Loss: 0.0308\n",
            "Epoch 13/20, Batch 104/764, Train Loss: 0.4184\n",
            "Epoch 13/20, Batch 112/764, Train Loss: 0.0046\n",
            "Epoch 13/20, Batch 120/764, Train Loss: 0.0004\n",
            "Epoch 13/20, Batch 128/764, Train Loss: 0.0875\n",
            "Epoch 13/20, Batch 136/764, Train Loss: 0.0612\n",
            "Epoch 13/20, Batch 144/764, Train Loss: 0.0005\n",
            "Epoch 13/20, Batch 152/764, Train Loss: 0.0004\n",
            "Epoch 13/20, Batch 160/764, Train Loss: 0.0005\n",
            "Epoch 13/20, Batch 168/764, Train Loss: 0.0005\n",
            "Epoch 13/20, Batch 176/764, Train Loss: 0.0012\n",
            "Epoch 13/20, Batch 184/764, Train Loss: 0.0067\n",
            "Epoch 13/20, Batch 192/764, Train Loss: 0.0097\n",
            "Epoch 13/20, Batch 200/764, Train Loss: 0.0012\n",
            "Epoch 13/20, Batch 208/764, Train Loss: 0.0672\n",
            "Epoch 13/20, Batch 216/764, Train Loss: 0.0027\n",
            "Epoch 13/20, Batch 224/764, Train Loss: 0.0007\n",
            "Epoch 13/20, Batch 232/764, Train Loss: 0.0005\n",
            "Epoch 13/20, Batch 240/764, Train Loss: 0.0076\n",
            "Epoch 13/20, Batch 248/764, Train Loss: 0.0012\n",
            "Epoch 13/20, Batch 256/764, Train Loss: 0.0002\n",
            "Epoch 13/20, Batch 264/764, Train Loss: 0.0002\n",
            "Epoch 13/20, Batch 272/764, Train Loss: 0.0286\n",
            "Epoch 13/20, Batch 280/764, Train Loss: 0.0146\n",
            "Epoch 13/20, Batch 288/764, Train Loss: 0.0010\n",
            "Epoch 13/20, Batch 296/764, Train Loss: 0.0013\n",
            "Epoch 13/20, Batch 304/764, Train Loss: 0.0416\n",
            "Epoch 13/20, Batch 312/764, Train Loss: 0.0230\n",
            "Epoch 13/20, Batch 320/764, Train Loss: 0.0230\n",
            "Epoch 13/20, Batch 328/764, Train Loss: 0.1458\n",
            "Epoch 13/20, Batch 336/764, Train Loss: 0.4972\n",
            "Epoch 13/20, Batch 344/764, Train Loss: 0.0003\n",
            "Epoch 13/20, Batch 352/764, Train Loss: 0.0765\n",
            "Epoch 13/20, Batch 360/764, Train Loss: 0.0006\n",
            "Epoch 13/20, Batch 368/764, Train Loss: 0.0023\n",
            "Epoch 13/20, Batch 376/764, Train Loss: 0.0010\n",
            "Epoch 13/20, Batch 384/764, Train Loss: 0.0062\n",
            "Epoch 13/20, Batch 392/764, Train Loss: 0.0059\n",
            "Epoch 13/20, Batch 400/764, Train Loss: 0.0531\n",
            "Epoch 13/20, Batch 408/764, Train Loss: 0.0013\n",
            "Epoch 13/20, Batch 416/764, Train Loss: 0.0113\n",
            "Epoch 13/20, Batch 424/764, Train Loss: 0.0768\n",
            "Epoch 13/20, Batch 432/764, Train Loss: 0.2767\n",
            "Epoch 13/20, Batch 440/764, Train Loss: 0.0013\n",
            "Epoch 13/20, Batch 448/764, Train Loss: 0.2524\n",
            "Epoch 13/20, Batch 456/764, Train Loss: 0.1790\n",
            "Epoch 13/20, Batch 464/764, Train Loss: 0.0127\n",
            "Epoch 13/20, Batch 472/764, Train Loss: 0.0008\n",
            "Epoch 13/20, Batch 480/764, Train Loss: 0.1597\n",
            "Epoch 13/20, Batch 488/764, Train Loss: 0.0127\n",
            "Epoch 13/20, Batch 496/764, Train Loss: 0.0089\n",
            "Epoch 13/20, Batch 504/764, Train Loss: 0.4233\n",
            "Epoch 13/20, Batch 512/764, Train Loss: 0.0122\n",
            "Epoch 13/20, Batch 520/764, Train Loss: 0.1394\n",
            "Epoch 13/20, Batch 528/764, Train Loss: 0.0464\n",
            "Epoch 13/20, Batch 536/764, Train Loss: 0.0081\n",
            "Epoch 13/20, Batch 544/764, Train Loss: 0.0070\n",
            "Epoch 13/20, Batch 552/764, Train Loss: 0.0117\n",
            "Epoch 13/20, Batch 560/764, Train Loss: 0.0084\n",
            "Epoch 13/20, Batch 568/764, Train Loss: 0.0009\n",
            "Epoch 13/20, Batch 576/764, Train Loss: 0.0099\n",
            "Epoch 13/20, Batch 584/764, Train Loss: 1.0792\n",
            "Epoch 13/20, Batch 592/764, Train Loss: 0.0043\n",
            "Epoch 13/20, Batch 600/764, Train Loss: 0.0089\n",
            "Epoch 13/20, Batch 608/764, Train Loss: 0.0819\n",
            "Epoch 13/20, Batch 616/764, Train Loss: 0.0025\n",
            "Epoch 13/20, Batch 624/764, Train Loss: 0.0025\n",
            "Epoch 13/20, Batch 632/764, Train Loss: 0.0259\n",
            "Epoch 13/20, Batch 640/764, Train Loss: 0.0212\n",
            "Epoch 13/20, Batch 648/764, Train Loss: 0.0010\n",
            "Epoch 13/20, Batch 656/764, Train Loss: 0.2168\n",
            "Epoch 13/20, Batch 664/764, Train Loss: 0.2303\n",
            "Epoch 13/20, Batch 672/764, Train Loss: 0.2443\n",
            "Epoch 13/20, Batch 680/764, Train Loss: 0.0041\n",
            "Epoch 13/20, Batch 688/764, Train Loss: 0.0252\n",
            "Epoch 13/20, Batch 696/764, Train Loss: 0.0156\n",
            "Epoch 13/20, Batch 704/764, Train Loss: 0.3006\n",
            "Epoch 13/20, Batch 712/764, Train Loss: 0.0008\n",
            "Epoch 13/20, Batch 720/764, Train Loss: 0.0163\n",
            "Epoch 13/20, Batch 728/764, Train Loss: 0.0624\n",
            "Epoch 13/20, Batch 736/764, Train Loss: 0.0017\n",
            "Epoch 13/20, Batch 744/764, Train Loss: 0.0442\n",
            "Epoch 13/20, Batch 752/764, Train Loss: 0.0075\n",
            "Epoch 13/20, Batch 760/764, Train Loss: 0.0173\n",
            "========================================================================================\n",
            "Epoch 13/20, Val Loss: 0.4601, Val Accuracy: 0.8893, Val F1: 0.8953, Best Accuracy: 0.9443\n",
            "========================================================================================\n",
            "Epoch 14/20, Batch 8/764, Train Loss: 0.0586\n",
            "Epoch 14/20, Batch 16/764, Train Loss: 0.0055\n",
            "Epoch 14/20, Batch 24/764, Train Loss: 0.0022\n",
            "Epoch 14/20, Batch 32/764, Train Loss: 0.2794\n",
            "Epoch 14/20, Batch 40/764, Train Loss: 0.0486\n",
            "Epoch 14/20, Batch 48/764, Train Loss: 0.0016\n",
            "Epoch 14/20, Batch 56/764, Train Loss: 0.5178\n",
            "Epoch 14/20, Batch 64/764, Train Loss: 0.0034\n",
            "Epoch 14/20, Batch 72/764, Train Loss: 0.0051\n",
            "Epoch 14/20, Batch 80/764, Train Loss: 0.3909\n",
            "Epoch 14/20, Batch 88/764, Train Loss: 0.0064\n",
            "Epoch 14/20, Batch 96/764, Train Loss: 0.0034\n",
            "Epoch 14/20, Batch 104/764, Train Loss: 0.0208\n",
            "Epoch 14/20, Batch 112/764, Train Loss: 0.0143\n",
            "Epoch 14/20, Batch 120/764, Train Loss: 0.0172\n",
            "Epoch 14/20, Batch 128/764, Train Loss: 0.0036\n",
            "Epoch 14/20, Batch 136/764, Train Loss: 0.0037\n",
            "Epoch 14/20, Batch 144/764, Train Loss: 0.2884\n",
            "Epoch 14/20, Batch 152/764, Train Loss: 0.0009\n",
            "Epoch 14/20, Batch 160/764, Train Loss: 0.0006\n",
            "Epoch 14/20, Batch 168/764, Train Loss: 0.0004\n",
            "Epoch 14/20, Batch 176/764, Train Loss: 0.0099\n",
            "Epoch 14/20, Batch 184/764, Train Loss: 0.0007\n",
            "Epoch 14/20, Batch 192/764, Train Loss: 0.0498\n",
            "Epoch 14/20, Batch 200/764, Train Loss: 0.0167\n",
            "Epoch 14/20, Batch 208/764, Train Loss: 0.0003\n",
            "Epoch 14/20, Batch 216/764, Train Loss: 0.0019\n",
            "Epoch 14/20, Batch 224/764, Train Loss: 0.0014\n",
            "Epoch 14/20, Batch 232/764, Train Loss: 0.0020\n",
            "Epoch 14/20, Batch 240/764, Train Loss: 0.0020\n",
            "Epoch 14/20, Batch 248/764, Train Loss: 0.0074\n",
            "Epoch 14/20, Batch 256/764, Train Loss: 0.0008\n",
            "Epoch 14/20, Batch 264/764, Train Loss: 0.6825\n",
            "Epoch 14/20, Batch 272/764, Train Loss: 0.0045\n",
            "Epoch 14/20, Batch 280/764, Train Loss: 0.0052\n",
            "Epoch 14/20, Batch 288/764, Train Loss: 0.0056\n",
            "Epoch 14/20, Batch 296/764, Train Loss: 0.0076\n",
            "Epoch 14/20, Batch 304/764, Train Loss: 0.0169\n",
            "Epoch 14/20, Batch 312/764, Train Loss: 0.2263\n",
            "Epoch 14/20, Batch 320/764, Train Loss: 0.1149\n",
            "Epoch 14/20, Batch 328/764, Train Loss: 0.1515\n",
            "Epoch 14/20, Batch 336/764, Train Loss: 0.0006\n",
            "Epoch 14/20, Batch 344/764, Train Loss: 0.0220\n",
            "Epoch 14/20, Batch 352/764, Train Loss: 0.0126\n",
            "Epoch 14/20, Batch 360/764, Train Loss: 0.1304\n",
            "Epoch 14/20, Batch 368/764, Train Loss: 0.0134\n",
            "Epoch 14/20, Batch 376/764, Train Loss: 1.8465\n",
            "Epoch 14/20, Batch 384/764, Train Loss: 0.0099\n",
            "Epoch 14/20, Batch 392/764, Train Loss: 0.0361\n",
            "Epoch 14/20, Batch 400/764, Train Loss: 0.2182\n",
            "Epoch 14/20, Batch 408/764, Train Loss: 0.0031\n",
            "Epoch 14/20, Batch 416/764, Train Loss: 0.0078\n",
            "Epoch 14/20, Batch 424/764, Train Loss: 0.1163\n",
            "Epoch 14/20, Batch 432/764, Train Loss: 0.8976\n",
            "Epoch 14/20, Batch 440/764, Train Loss: 0.0430\n",
            "Epoch 14/20, Batch 448/764, Train Loss: 0.0218\n",
            "Epoch 14/20, Batch 456/764, Train Loss: 0.0236\n",
            "Epoch 14/20, Batch 464/764, Train Loss: 0.0154\n",
            "Epoch 14/20, Batch 472/764, Train Loss: 0.1571\n",
            "Epoch 14/20, Batch 480/764, Train Loss: 0.6220\n",
            "Epoch 14/20, Batch 488/764, Train Loss: 0.0112\n",
            "Epoch 14/20, Batch 496/764, Train Loss: 0.0644\n",
            "Epoch 14/20, Batch 504/764, Train Loss: 0.0439\n",
            "Epoch 14/20, Batch 512/764, Train Loss: 0.6773\n",
            "Epoch 14/20, Batch 520/764, Train Loss: 0.0122\n",
            "Epoch 14/20, Batch 528/764, Train Loss: 0.0092\n",
            "Epoch 14/20, Batch 536/764, Train Loss: 0.0077\n",
            "Epoch 14/20, Batch 544/764, Train Loss: 0.0101\n",
            "Epoch 14/20, Batch 552/764, Train Loss: 0.0025\n",
            "Epoch 14/20, Batch 560/764, Train Loss: 0.0043\n",
            "Epoch 14/20, Batch 568/764, Train Loss: 0.0164\n",
            "Epoch 14/20, Batch 576/764, Train Loss: 0.0030\n",
            "Epoch 14/20, Batch 584/764, Train Loss: 0.2663\n",
            "Epoch 14/20, Batch 592/764, Train Loss: 0.0056\n",
            "Epoch 14/20, Batch 600/764, Train Loss: 0.0018\n",
            "Epoch 14/20, Batch 608/764, Train Loss: 0.1144\n",
            "Epoch 14/20, Batch 616/764, Train Loss: 0.0009\n",
            "Epoch 14/20, Batch 624/764, Train Loss: 0.0008\n",
            "Epoch 14/20, Batch 632/764, Train Loss: 0.0240\n",
            "Epoch 14/20, Batch 640/764, Train Loss: 0.0004\n",
            "Epoch 14/20, Batch 648/764, Train Loss: 0.0009\n",
            "Epoch 14/20, Batch 656/764, Train Loss: 0.0067\n",
            "Epoch 14/20, Batch 664/764, Train Loss: 0.1226\n",
            "Epoch 14/20, Batch 672/764, Train Loss: 0.0076\n",
            "Epoch 14/20, Batch 680/764, Train Loss: 1.0564\n",
            "Epoch 14/20, Batch 688/764, Train Loss: 0.0737\n",
            "Epoch 14/20, Batch 696/764, Train Loss: 0.0077\n",
            "Epoch 14/20, Batch 704/764, Train Loss: 0.0014\n",
            "Epoch 14/20, Batch 712/764, Train Loss: 0.0003\n",
            "Epoch 14/20, Batch 720/764, Train Loss: 0.0011\n",
            "Epoch 14/20, Batch 728/764, Train Loss: 0.0005\n",
            "Epoch 14/20, Batch 736/764, Train Loss: 0.0150\n",
            "Epoch 14/20, Batch 744/764, Train Loss: 0.0004\n",
            "Epoch 14/20, Batch 752/764, Train Loss: 0.0002\n",
            "Epoch 14/20, Batch 760/764, Train Loss: 0.0009\n",
            "========================================================================================\n",
            "Epoch 14/20, Val Loss: 0.3909, Val Accuracy: 0.9260, Val F1: 0.9326, Best Accuracy: 0.9443\n",
            "========================================================================================\n",
            "Epoch 15/20, Batch 8/764, Train Loss: 0.0360\n",
            "Epoch 15/20, Batch 16/764, Train Loss: 0.0017\n",
            "Epoch 15/20, Batch 24/764, Train Loss: 0.0044\n",
            "Epoch 15/20, Batch 32/764, Train Loss: 0.0294\n",
            "Epoch 15/20, Batch 40/764, Train Loss: 0.0043\n",
            "Epoch 15/20, Batch 48/764, Train Loss: 0.0197\n",
            "Epoch 15/20, Batch 56/764, Train Loss: 0.0923\n",
            "Epoch 15/20, Batch 64/764, Train Loss: 0.1928\n",
            "Epoch 15/20, Batch 72/764, Train Loss: 0.0020\n",
            "Epoch 15/20, Batch 80/764, Train Loss: 0.2197\n",
            "Epoch 15/20, Batch 88/764, Train Loss: 0.0287\n",
            "Epoch 15/20, Batch 96/764, Train Loss: 0.0238\n",
            "Epoch 15/20, Batch 104/764, Train Loss: 0.1600\n",
            "Epoch 15/20, Batch 112/764, Train Loss: 0.5893\n",
            "Epoch 15/20, Batch 120/764, Train Loss: 0.3874\n",
            "Epoch 15/20, Batch 128/764, Train Loss: 0.0065\n",
            "Epoch 15/20, Batch 136/764, Train Loss: 0.1627\n",
            "Epoch 15/20, Batch 144/764, Train Loss: 0.0057\n",
            "Epoch 15/20, Batch 152/764, Train Loss: 0.0236\n",
            "Epoch 15/20, Batch 160/764, Train Loss: 0.0020\n",
            "Epoch 15/20, Batch 168/764, Train Loss: 0.0022\n",
            "Epoch 15/20, Batch 176/764, Train Loss: 0.0011\n",
            "Epoch 15/20, Batch 184/764, Train Loss: 0.0174\n",
            "Epoch 15/20, Batch 192/764, Train Loss: 0.0004\n",
            "Epoch 15/20, Batch 200/764, Train Loss: 0.0408\n",
            "Epoch 15/20, Batch 208/764, Train Loss: 0.0054\n",
            "Epoch 15/20, Batch 216/764, Train Loss: 0.0010\n",
            "Epoch 15/20, Batch 224/764, Train Loss: 0.1967\n",
            "Epoch 15/20, Batch 232/764, Train Loss: 0.0010\n",
            "Epoch 15/20, Batch 240/764, Train Loss: 0.0096\n",
            "Epoch 15/20, Batch 248/764, Train Loss: 0.0018\n",
            "Epoch 15/20, Batch 256/764, Train Loss: 0.0009\n",
            "Epoch 15/20, Batch 264/764, Train Loss: 0.0023\n",
            "Epoch 15/20, Batch 272/764, Train Loss: 0.0012\n",
            "Epoch 15/20, Batch 280/764, Train Loss: 0.0007\n",
            "Epoch 15/20, Batch 288/764, Train Loss: 0.0013\n",
            "Epoch 15/20, Batch 296/764, Train Loss: 0.0014\n",
            "Epoch 15/20, Batch 304/764, Train Loss: 0.0008\n",
            "Epoch 15/20, Batch 312/764, Train Loss: 0.0002\n",
            "Epoch 15/20, Batch 320/764, Train Loss: 0.0014\n",
            "Epoch 15/20, Batch 328/764, Train Loss: 0.0035\n",
            "Epoch 15/20, Batch 336/764, Train Loss: 0.0008\n",
            "Epoch 15/20, Batch 344/764, Train Loss: 0.0004\n",
            "Epoch 15/20, Batch 352/764, Train Loss: 0.0017\n",
            "Epoch 15/20, Batch 360/764, Train Loss: 0.0006\n",
            "Epoch 15/20, Batch 368/764, Train Loss: 0.0131\n",
            "Epoch 15/20, Batch 376/764, Train Loss: 0.0021\n",
            "Epoch 15/20, Batch 384/764, Train Loss: 0.0007\n",
            "Epoch 15/20, Batch 392/764, Train Loss: 0.0021\n",
            "Epoch 15/20, Batch 400/764, Train Loss: 0.0006\n",
            "Epoch 15/20, Batch 408/764, Train Loss: 0.0013\n",
            "Epoch 15/20, Batch 416/764, Train Loss: 0.0007\n",
            "Epoch 15/20, Batch 424/764, Train Loss: 0.0008\n",
            "Epoch 15/20, Batch 432/764, Train Loss: 0.0312\n",
            "Epoch 15/20, Batch 440/764, Train Loss: 0.0381\n",
            "Epoch 15/20, Batch 448/764, Train Loss: 0.0074\n",
            "Epoch 15/20, Batch 456/764, Train Loss: 0.0015\n",
            "Epoch 15/20, Batch 464/764, Train Loss: 0.0009\n",
            "Epoch 15/20, Batch 472/764, Train Loss: 0.0001\n",
            "Epoch 15/20, Batch 480/764, Train Loss: 0.0001\n",
            "Epoch 15/20, Batch 488/764, Train Loss: 0.0008\n",
            "Epoch 15/20, Batch 496/764, Train Loss: 0.0017\n",
            "Epoch 15/20, Batch 504/764, Train Loss: 0.0008\n",
            "Epoch 15/20, Batch 512/764, Train Loss: 0.0003\n",
            "Epoch 15/20, Batch 520/764, Train Loss: 0.0009\n",
            "Epoch 15/20, Batch 528/764, Train Loss: 0.0040\n",
            "Epoch 15/20, Batch 536/764, Train Loss: 0.0002\n",
            "Epoch 15/20, Batch 544/764, Train Loss: 0.0002\n",
            "Epoch 15/20, Batch 552/764, Train Loss: 0.0005\n",
            "Epoch 15/20, Batch 560/764, Train Loss: 0.0002\n",
            "Epoch 15/20, Batch 568/764, Train Loss: 0.0026\n",
            "Epoch 15/20, Batch 576/764, Train Loss: 0.0008\n",
            "Epoch 15/20, Batch 584/764, Train Loss: 0.0921\n",
            "Epoch 15/20, Batch 592/764, Train Loss: 0.3173\n",
            "Epoch 15/20, Batch 600/764, Train Loss: 0.0002\n",
            "Epoch 15/20, Batch 608/764, Train Loss: 0.3097\n",
            "Epoch 15/20, Batch 616/764, Train Loss: 0.0004\n",
            "Epoch 15/20, Batch 624/764, Train Loss: 0.0065\n",
            "Epoch 15/20, Batch 632/764, Train Loss: 0.0004\n",
            "Epoch 15/20, Batch 640/764, Train Loss: 0.0997\n",
            "Epoch 15/20, Batch 648/764, Train Loss: 0.0020\n",
            "Epoch 15/20, Batch 656/764, Train Loss: 0.0067\n",
            "Epoch 15/20, Batch 664/764, Train Loss: 0.0107\n",
            "Epoch 15/20, Batch 672/764, Train Loss: 0.0066\n",
            "Epoch 15/20, Batch 680/764, Train Loss: 0.0005\n",
            "Epoch 15/20, Batch 688/764, Train Loss: 0.1357\n",
            "Epoch 15/20, Batch 696/764, Train Loss: 0.0007\n",
            "Epoch 15/20, Batch 704/764, Train Loss: 0.0020\n",
            "Epoch 15/20, Batch 712/764, Train Loss: 0.2508\n",
            "Epoch 15/20, Batch 720/764, Train Loss: 0.0091\n",
            "Epoch 15/20, Batch 728/764, Train Loss: 0.5049\n",
            "Epoch 15/20, Batch 736/764, Train Loss: 0.0017\n",
            "Epoch 15/20, Batch 744/764, Train Loss: 0.0005\n",
            "Epoch 15/20, Batch 752/764, Train Loss: 0.0002\n",
            "Epoch 15/20, Batch 760/764, Train Loss: 0.2575\n",
            "========================================================================================\n",
            "Epoch 15/20, Val Loss: 0.3632, Val Accuracy: 0.9160, Val F1: 0.9243, Best Accuracy: 0.9443\n",
            "========================================================================================\n",
            "Epoch 16/20, Batch 8/764, Train Loss: 0.0017\n",
            "Epoch 16/20, Batch 16/764, Train Loss: 0.0019\n",
            "Epoch 16/20, Batch 24/764, Train Loss: 0.0117\n",
            "Epoch 16/20, Batch 32/764, Train Loss: 0.0005\n",
            "Epoch 16/20, Batch 40/764, Train Loss: 0.0020\n",
            "Epoch 16/20, Batch 48/764, Train Loss: 0.0004\n",
            "Epoch 16/20, Batch 56/764, Train Loss: 0.0028\n",
            "Epoch 16/20, Batch 64/764, Train Loss: 0.0043\n",
            "Epoch 16/20, Batch 72/764, Train Loss: 0.0002\n",
            "Epoch 16/20, Batch 80/764, Train Loss: 0.0032\n",
            "Epoch 16/20, Batch 88/764, Train Loss: 0.0282\n",
            "Epoch 16/20, Batch 96/764, Train Loss: 0.0491\n",
            "Epoch 16/20, Batch 104/764, Train Loss: 0.0014\n",
            "Epoch 16/20, Batch 112/764, Train Loss: 0.0006\n",
            "Epoch 16/20, Batch 120/764, Train Loss: 0.0003\n",
            "Epoch 16/20, Batch 128/764, Train Loss: 0.5260\n",
            "Epoch 16/20, Batch 136/764, Train Loss: 0.7006\n",
            "Epoch 16/20, Batch 144/764, Train Loss: 0.1798\n",
            "Epoch 16/20, Batch 152/764, Train Loss: 0.1155\n",
            "Epoch 16/20, Batch 160/764, Train Loss: 0.0089\n",
            "Epoch 16/20, Batch 168/764, Train Loss: 0.0035\n",
            "Epoch 16/20, Batch 176/764, Train Loss: 0.0011\n",
            "Epoch 16/20, Batch 184/764, Train Loss: 0.3474\n",
            "Epoch 16/20, Batch 192/764, Train Loss: 0.0044\n",
            "Epoch 16/20, Batch 200/764, Train Loss: 0.0065\n",
            "Epoch 16/20, Batch 208/764, Train Loss: 0.0073\n",
            "Epoch 16/20, Batch 216/764, Train Loss: 0.0012\n",
            "Epoch 16/20, Batch 224/764, Train Loss: 0.0007\n",
            "Epoch 16/20, Batch 232/764, Train Loss: 0.0002\n",
            "Epoch 16/20, Batch 240/764, Train Loss: 0.0334\n",
            "Epoch 16/20, Batch 248/764, Train Loss: 0.0764\n",
            "Epoch 16/20, Batch 256/764, Train Loss: 0.3473\n",
            "Epoch 16/20, Batch 264/764, Train Loss: 0.0023\n",
            "Epoch 16/20, Batch 272/764, Train Loss: 0.0036\n",
            "Epoch 16/20, Batch 280/764, Train Loss: 0.0156\n",
            "Epoch 16/20, Batch 288/764, Train Loss: 0.1139\n",
            "Epoch 16/20, Batch 296/764, Train Loss: 0.0027\n",
            "Epoch 16/20, Batch 304/764, Train Loss: 0.1078\n",
            "Epoch 16/20, Batch 312/764, Train Loss: 0.0005\n",
            "Epoch 16/20, Batch 320/764, Train Loss: 0.0003\n",
            "Epoch 16/20, Batch 328/764, Train Loss: 0.0003\n",
            "Epoch 16/20, Batch 336/764, Train Loss: 0.0026\n",
            "Epoch 16/20, Batch 344/764, Train Loss: 0.0033\n",
            "Epoch 16/20, Batch 352/764, Train Loss: 0.0085\n",
            "Epoch 16/20, Batch 360/764, Train Loss: 0.0014\n",
            "Epoch 16/20, Batch 368/764, Train Loss: 0.0001\n",
            "Epoch 16/20, Batch 376/764, Train Loss: 0.0008\n",
            "Epoch 16/20, Batch 384/764, Train Loss: 0.0027\n",
            "Epoch 16/20, Batch 392/764, Train Loss: 0.2497\n",
            "Epoch 16/20, Batch 400/764, Train Loss: 0.0005\n",
            "Epoch 16/20, Batch 408/764, Train Loss: 0.0004\n",
            "Epoch 16/20, Batch 416/764, Train Loss: 0.0002\n",
            "Epoch 16/20, Batch 424/764, Train Loss: 0.0003\n",
            "Epoch 16/20, Batch 432/764, Train Loss: 0.6506\n",
            "Epoch 16/20, Batch 440/764, Train Loss: 0.0005\n",
            "Epoch 16/20, Batch 448/764, Train Loss: 0.0522\n",
            "Epoch 16/20, Batch 456/764, Train Loss: 0.0871\n",
            "Epoch 16/20, Batch 464/764, Train Loss: 0.0246\n",
            "Epoch 16/20, Batch 472/764, Train Loss: 0.0870\n",
            "Epoch 16/20, Batch 480/764, Train Loss: 0.0038\n",
            "Epoch 16/20, Batch 488/764, Train Loss: 0.0011\n",
            "Epoch 16/20, Batch 496/764, Train Loss: 0.0523\n",
            "Epoch 16/20, Batch 504/764, Train Loss: 0.0100\n",
            "Epoch 16/20, Batch 512/764, Train Loss: 0.0044\n",
            "Epoch 16/20, Batch 520/764, Train Loss: 0.0025\n",
            "Epoch 16/20, Batch 528/764, Train Loss: 0.0901\n",
            "Epoch 16/20, Batch 536/764, Train Loss: 0.0024\n",
            "Epoch 16/20, Batch 544/764, Train Loss: 0.0008\n",
            "Epoch 16/20, Batch 552/764, Train Loss: 0.0003\n",
            "Epoch 16/20, Batch 560/764, Train Loss: 0.0119\n",
            "Epoch 16/20, Batch 568/764, Train Loss: 0.0600\n",
            "Epoch 16/20, Batch 576/764, Train Loss: 0.0003\n",
            "Epoch 16/20, Batch 584/764, Train Loss: 0.0010\n",
            "Epoch 16/20, Batch 592/764, Train Loss: 0.0003\n",
            "Epoch 16/20, Batch 600/764, Train Loss: 0.0057\n",
            "Epoch 16/20, Batch 608/764, Train Loss: 0.0022\n",
            "Epoch 16/20, Batch 616/764, Train Loss: 0.0763\n",
            "Epoch 16/20, Batch 624/764, Train Loss: 0.0249\n",
            "Epoch 16/20, Batch 632/764, Train Loss: 0.0001\n",
            "Epoch 16/20, Batch 640/764, Train Loss: 0.0174\n",
            "Epoch 16/20, Batch 648/764, Train Loss: 0.0066\n",
            "Epoch 16/20, Batch 656/764, Train Loss: 0.0037\n",
            "Epoch 16/20, Batch 664/764, Train Loss: 0.0015\n",
            "Epoch 16/20, Batch 672/764, Train Loss: 0.0129\n",
            "Epoch 16/20, Batch 680/764, Train Loss: 0.0358\n",
            "Epoch 16/20, Batch 688/764, Train Loss: 0.0001\n",
            "Epoch 16/20, Batch 696/764, Train Loss: 0.2679\n",
            "Epoch 16/20, Batch 704/764, Train Loss: 0.0122\n",
            "Epoch 16/20, Batch 712/764, Train Loss: 0.0017\n",
            "Epoch 16/20, Batch 720/764, Train Loss: 1.4923\n",
            "Epoch 16/20, Batch 728/764, Train Loss: 0.0001\n",
            "Epoch 16/20, Batch 736/764, Train Loss: 0.0002\n",
            "Epoch 16/20, Batch 744/764, Train Loss: 0.0001\n",
            "Epoch 16/20, Batch 752/764, Train Loss: 0.0048\n",
            "Epoch 16/20, Batch 760/764, Train Loss: 0.0014\n",
            "========================================================================================\n",
            "Epoch 16/20, Val Loss: 0.3135, Val Accuracy: 0.9359, Val F1: 0.9413, Best Accuracy: 0.9443\n",
            "========================================================================================\n",
            "Epoch 17/20, Batch 8/764, Train Loss: 0.0893\n",
            "Epoch 17/20, Batch 16/764, Train Loss: 0.0004\n",
            "Epoch 17/20, Batch 24/764, Train Loss: 0.0015\n",
            "Epoch 17/20, Batch 32/764, Train Loss: 0.0004\n",
            "Epoch 17/20, Batch 40/764, Train Loss: 0.0003\n",
            "Epoch 17/20, Batch 48/764, Train Loss: 0.0021\n",
            "Epoch 17/20, Batch 56/764, Train Loss: 0.0340\n",
            "Epoch 17/20, Batch 64/764, Train Loss: 0.0002\n",
            "Epoch 17/20, Batch 72/764, Train Loss: 0.5593\n",
            "Epoch 17/20, Batch 80/764, Train Loss: 0.0022\n",
            "Epoch 17/20, Batch 88/764, Train Loss: 0.0083\n",
            "Epoch 17/20, Batch 96/764, Train Loss: 0.0007\n",
            "Epoch 17/20, Batch 104/764, Train Loss: 0.0413\n",
            "Epoch 17/20, Batch 112/764, Train Loss: 0.0002\n",
            "Epoch 17/20, Batch 120/764, Train Loss: 0.0069\n",
            "Epoch 17/20, Batch 128/764, Train Loss: 0.0001\n",
            "Epoch 17/20, Batch 136/764, Train Loss: 0.0037\n",
            "Epoch 17/20, Batch 144/764, Train Loss: 0.0060\n",
            "Epoch 17/20, Batch 152/764, Train Loss: 0.0200\n",
            "Epoch 17/20, Batch 160/764, Train Loss: 0.0002\n",
            "Epoch 17/20, Batch 168/764, Train Loss: 0.1093\n",
            "Epoch 17/20, Batch 176/764, Train Loss: 0.0988\n",
            "Epoch 17/20, Batch 184/764, Train Loss: 0.0013\n",
            "Epoch 17/20, Batch 192/764, Train Loss: 0.0009\n",
            "Epoch 17/20, Batch 200/764, Train Loss: 0.9730\n",
            "Epoch 17/20, Batch 208/764, Train Loss: 0.0038\n",
            "Epoch 17/20, Batch 216/764, Train Loss: 0.0001\n",
            "Epoch 17/20, Batch 224/764, Train Loss: 0.0011\n",
            "Epoch 17/20, Batch 232/764, Train Loss: 0.0003\n",
            "Epoch 17/20, Batch 240/764, Train Loss: 0.0002\n",
            "Epoch 17/20, Batch 248/764, Train Loss: 0.0002\n",
            "Epoch 17/20, Batch 256/764, Train Loss: 0.0016\n",
            "Epoch 17/20, Batch 264/764, Train Loss: 0.0003\n",
            "Epoch 17/20, Batch 272/764, Train Loss: 0.0001\n",
            "Epoch 17/20, Batch 280/764, Train Loss: 0.0001\n",
            "Epoch 17/20, Batch 288/764, Train Loss: 0.0016\n",
            "Epoch 17/20, Batch 296/764, Train Loss: 0.0851\n",
            "Epoch 17/20, Batch 304/764, Train Loss: 0.0001\n",
            "Epoch 17/20, Batch 312/764, Train Loss: 0.0002\n",
            "Epoch 17/20, Batch 320/764, Train Loss: 0.0002\n",
            "Epoch 17/20, Batch 328/764, Train Loss: 0.0011\n",
            "Epoch 17/20, Batch 336/764, Train Loss: 0.0548\n",
            "Epoch 17/20, Batch 344/764, Train Loss: 0.0010\n",
            "Epoch 17/20, Batch 352/764, Train Loss: 0.0006\n",
            "Epoch 17/20, Batch 360/764, Train Loss: 0.0031\n",
            "Epoch 17/20, Batch 368/764, Train Loss: 0.0020\n",
            "Epoch 17/20, Batch 376/764, Train Loss: 0.0009\n",
            "Epoch 17/20, Batch 384/764, Train Loss: 0.0043\n",
            "Epoch 17/20, Batch 392/764, Train Loss: 0.0052\n",
            "Epoch 17/20, Batch 400/764, Train Loss: 0.0004\n",
            "Epoch 17/20, Batch 408/764, Train Loss: 0.2554\n",
            "Epoch 17/20, Batch 416/764, Train Loss: 0.0167\n",
            "Epoch 17/20, Batch 424/764, Train Loss: 0.0342\n",
            "Epoch 17/20, Batch 432/764, Train Loss: 0.0048\n",
            "Epoch 17/20, Batch 440/764, Train Loss: 0.0098\n",
            "Epoch 17/20, Batch 448/764, Train Loss: 0.0033\n",
            "Epoch 17/20, Batch 456/764, Train Loss: 0.1031\n",
            "Epoch 17/20, Batch 464/764, Train Loss: 0.0591\n",
            "Epoch 17/20, Batch 472/764, Train Loss: 0.0105\n",
            "Epoch 17/20, Batch 480/764, Train Loss: 0.0049\n",
            "Epoch 17/20, Batch 488/764, Train Loss: 0.0129\n",
            "Epoch 17/20, Batch 496/764, Train Loss: 0.0009\n",
            "Epoch 17/20, Batch 504/764, Train Loss: 0.0026\n",
            "Epoch 17/20, Batch 512/764, Train Loss: 0.0054\n",
            "Epoch 17/20, Batch 520/764, Train Loss: 0.0001\n",
            "Epoch 17/20, Batch 528/764, Train Loss: 0.0152\n",
            "Epoch 17/20, Batch 536/764, Train Loss: 0.0259\n",
            "Epoch 17/20, Batch 544/764, Train Loss: 0.0004\n",
            "Epoch 17/20, Batch 552/764, Train Loss: 0.0037\n",
            "Epoch 17/20, Batch 560/764, Train Loss: 0.0130\n",
            "Epoch 17/20, Batch 568/764, Train Loss: 0.0522\n",
            "Epoch 17/20, Batch 576/764, Train Loss: 0.0055\n",
            "Epoch 17/20, Batch 584/764, Train Loss: 0.0006\n",
            "Epoch 17/20, Batch 592/764, Train Loss: 0.0018\n",
            "Epoch 17/20, Batch 600/764, Train Loss: 0.0036\n",
            "Epoch 17/20, Batch 608/764, Train Loss: 0.0013\n",
            "Epoch 17/20, Batch 616/764, Train Loss: 0.0045\n",
            "Epoch 17/20, Batch 624/764, Train Loss: 0.0032\n",
            "Epoch 17/20, Batch 632/764, Train Loss: 0.0337\n",
            "Epoch 17/20, Batch 640/764, Train Loss: 0.0027\n",
            "Epoch 17/20, Batch 648/764, Train Loss: 0.0072\n",
            "Epoch 17/20, Batch 656/764, Train Loss: 0.0136\n",
            "Epoch 17/20, Batch 664/764, Train Loss: 0.0020\n",
            "Epoch 17/20, Batch 672/764, Train Loss: 0.0008\n",
            "Epoch 17/20, Batch 680/764, Train Loss: 0.0002\n",
            "Epoch 17/20, Batch 688/764, Train Loss: 0.0001\n",
            "Epoch 17/20, Batch 696/764, Train Loss: 0.0042\n",
            "Epoch 17/20, Batch 704/764, Train Loss: 0.0054\n",
            "Epoch 17/20, Batch 712/764, Train Loss: 0.0124\n",
            "Epoch 17/20, Batch 720/764, Train Loss: 0.0012\n",
            "Epoch 17/20, Batch 728/764, Train Loss: 0.0676\n",
            "Epoch 17/20, Batch 736/764, Train Loss: 0.0037\n",
            "Epoch 17/20, Batch 744/764, Train Loss: 0.0190\n",
            "Epoch 17/20, Batch 752/764, Train Loss: 0.0002\n",
            "Epoch 17/20, Batch 760/764, Train Loss: 0.0004\n",
            "========================================================================================\n",
            "Epoch 17/20, Val Loss: 0.2929, Val Accuracy: 0.9374, Val F1: 0.9403, Best Accuracy: 0.9443\n",
            "========================================================================================\n",
            "Epoch 18/20, Batch 8/764, Train Loss: 0.2248\n",
            "Epoch 18/20, Batch 16/764, Train Loss: 0.0006\n",
            "Epoch 18/20, Batch 24/764, Train Loss: 0.0007\n",
            "Epoch 18/20, Batch 32/764, Train Loss: 0.0085\n",
            "Epoch 18/20, Batch 40/764, Train Loss: 0.0001\n",
            "Epoch 18/20, Batch 48/764, Train Loss: 0.0037\n",
            "Epoch 18/20, Batch 56/764, Train Loss: 0.0002\n",
            "Epoch 18/20, Batch 64/764, Train Loss: 0.0001\n",
            "Epoch 18/20, Batch 72/764, Train Loss: 0.0003\n",
            "Epoch 18/20, Batch 80/764, Train Loss: 0.0006\n",
            "Epoch 18/20, Batch 88/764, Train Loss: 0.0001\n",
            "Epoch 18/20, Batch 96/764, Train Loss: 0.0002\n",
            "Epoch 18/20, Batch 104/764, Train Loss: 0.0002\n",
            "Epoch 18/20, Batch 112/764, Train Loss: 0.0003\n",
            "Epoch 18/20, Batch 120/764, Train Loss: 0.0005\n",
            "Epoch 18/20, Batch 128/764, Train Loss: 0.0001\n",
            "Epoch 18/20, Batch 136/764, Train Loss: 0.0001\n",
            "Epoch 18/20, Batch 144/764, Train Loss: 0.0018\n",
            "Epoch 18/20, Batch 152/764, Train Loss: 0.0002\n",
            "Epoch 18/20, Batch 160/764, Train Loss: 0.0000\n",
            "Epoch 18/20, Batch 168/764, Train Loss: 0.0007\n",
            "Epoch 18/20, Batch 176/764, Train Loss: 0.0279\n",
            "Epoch 18/20, Batch 184/764, Train Loss: 0.0001\n",
            "Epoch 18/20, Batch 192/764, Train Loss: 0.0002\n",
            "Epoch 18/20, Batch 200/764, Train Loss: 0.0313\n",
            "Epoch 18/20, Batch 208/764, Train Loss: 0.0019\n",
            "Epoch 18/20, Batch 216/764, Train Loss: 0.4491\n",
            "Epoch 18/20, Batch 224/764, Train Loss: 0.0047\n",
            "Epoch 18/20, Batch 232/764, Train Loss: 0.0135\n",
            "Epoch 18/20, Batch 240/764, Train Loss: 0.0054\n",
            "Epoch 18/20, Batch 248/764, Train Loss: 0.0675\n",
            "Epoch 18/20, Batch 256/764, Train Loss: 0.2118\n",
            "Epoch 18/20, Batch 264/764, Train Loss: 0.0005\n",
            "Epoch 18/20, Batch 272/764, Train Loss: 0.0485\n",
            "Epoch 18/20, Batch 280/764, Train Loss: 0.0001\n",
            "Epoch 18/20, Batch 288/764, Train Loss: 0.0019\n",
            "Epoch 18/20, Batch 296/764, Train Loss: 0.2179\n",
            "Epoch 18/20, Batch 304/764, Train Loss: 0.0001\n",
            "Epoch 18/20, Batch 312/764, Train Loss: 0.2202\n",
            "Epoch 18/20, Batch 320/764, Train Loss: 0.0003\n",
            "Epoch 18/20, Batch 328/764, Train Loss: 0.0542\n",
            "Epoch 18/20, Batch 336/764, Train Loss: 0.0371\n",
            "Epoch 18/20, Batch 344/764, Train Loss: 0.0738\n",
            "Epoch 18/20, Batch 352/764, Train Loss: 0.0167\n",
            "Epoch 18/20, Batch 360/764, Train Loss: 0.2536\n",
            "Epoch 18/20, Batch 368/764, Train Loss: 0.0002\n",
            "Epoch 18/20, Batch 376/764, Train Loss: 0.0055\n",
            "Epoch 18/20, Batch 384/764, Train Loss: 0.0039\n",
            "Epoch 18/20, Batch 392/764, Train Loss: 0.0017\n",
            "Epoch 18/20, Batch 400/764, Train Loss: 0.0026\n",
            "Epoch 18/20, Batch 408/764, Train Loss: 0.0004\n",
            "Epoch 18/20, Batch 416/764, Train Loss: 0.0001\n",
            "Epoch 18/20, Batch 424/764, Train Loss: 0.0008\n",
            "Epoch 18/20, Batch 432/764, Train Loss: 0.0017\n",
            "Epoch 18/20, Batch 440/764, Train Loss: 0.3013\n",
            "Epoch 18/20, Batch 448/764, Train Loss: 0.0027\n",
            "Epoch 18/20, Batch 456/764, Train Loss: 0.0036\n",
            "Epoch 18/20, Batch 464/764, Train Loss: 0.2723\n",
            "Epoch 18/20, Batch 472/764, Train Loss: 0.0032\n",
            "Epoch 18/20, Batch 480/764, Train Loss: 0.0051\n",
            "Epoch 18/20, Batch 488/764, Train Loss: 0.0065\n",
            "Epoch 18/20, Batch 496/764, Train Loss: 0.0677\n",
            "Epoch 18/20, Batch 504/764, Train Loss: 0.0024\n",
            "Epoch 18/20, Batch 512/764, Train Loss: 0.0020\n",
            "Epoch 18/20, Batch 520/764, Train Loss: 0.0008\n",
            "Epoch 18/20, Batch 528/764, Train Loss: 0.0007\n",
            "Epoch 18/20, Batch 536/764, Train Loss: 0.0045\n",
            "Epoch 18/20, Batch 544/764, Train Loss: 0.1657\n",
            "Epoch 18/20, Batch 552/764, Train Loss: 0.0153\n",
            "Epoch 18/20, Batch 560/764, Train Loss: 0.0003\n",
            "Epoch 18/20, Batch 568/764, Train Loss: 0.6653\n",
            "Epoch 18/20, Batch 576/764, Train Loss: 0.0012\n",
            "Epoch 18/20, Batch 584/764, Train Loss: 0.0034\n",
            "Epoch 18/20, Batch 592/764, Train Loss: 0.0035\n",
            "Epoch 18/20, Batch 600/764, Train Loss: 0.0025\n",
            "Epoch 18/20, Batch 608/764, Train Loss: 0.0033\n",
            "Epoch 18/20, Batch 616/764, Train Loss: 0.0029\n",
            "Epoch 18/20, Batch 624/764, Train Loss: 0.0012\n",
            "Epoch 18/20, Batch 632/764, Train Loss: 0.0336\n",
            "Epoch 18/20, Batch 640/764, Train Loss: 0.0022\n",
            "Epoch 18/20, Batch 648/764, Train Loss: 0.0009\n",
            "Epoch 18/20, Batch 656/764, Train Loss: 0.0026\n",
            "Epoch 18/20, Batch 664/764, Train Loss: 0.5220\n",
            "Epoch 18/20, Batch 672/764, Train Loss: 0.0415\n",
            "Epoch 18/20, Batch 680/764, Train Loss: 0.0017\n",
            "Epoch 18/20, Batch 688/764, Train Loss: 0.3721\n",
            "Epoch 18/20, Batch 696/764, Train Loss: 0.0494\n",
            "Epoch 18/20, Batch 704/764, Train Loss: 0.3378\n",
            "Epoch 18/20, Batch 712/764, Train Loss: 0.0086\n",
            "Epoch 18/20, Batch 720/764, Train Loss: 0.0083\n",
            "Epoch 18/20, Batch 728/764, Train Loss: 0.2457\n",
            "Epoch 18/20, Batch 736/764, Train Loss: 0.0133\n",
            "Epoch 18/20, Batch 744/764, Train Loss: 0.0055\n",
            "Epoch 18/20, Batch 752/764, Train Loss: 0.0033\n",
            "Epoch 18/20, Batch 760/764, Train Loss: 0.0764\n",
            "========================================================================================\n",
            "Epoch 18/20, Val Loss: 0.2708, Val Accuracy: 0.9344, Val F1: 0.9390, Best Accuracy: 0.9443\n",
            "========================================================================================\n",
            "Epoch 19/20, Batch 8/764, Train Loss: 0.0053\n",
            "Epoch 19/20, Batch 16/764, Train Loss: 0.0034\n",
            "Epoch 19/20, Batch 24/764, Train Loss: 0.0104\n",
            "Epoch 19/20, Batch 32/764, Train Loss: 0.0135\n",
            "Epoch 19/20, Batch 40/764, Train Loss: 0.0063\n",
            "Epoch 19/20, Batch 48/764, Train Loss: 0.0241\n",
            "Epoch 19/20, Batch 56/764, Train Loss: 0.0212\n",
            "Epoch 19/20, Batch 64/764, Train Loss: 0.0026\n",
            "Epoch 19/20, Batch 72/764, Train Loss: 0.0446\n",
            "Epoch 19/20, Batch 80/764, Train Loss: 0.0024\n",
            "Epoch 19/20, Batch 88/764, Train Loss: 0.0023\n",
            "Epoch 19/20, Batch 96/764, Train Loss: 0.0157\n",
            "Epoch 19/20, Batch 104/764, Train Loss: 0.0019\n",
            "Epoch 19/20, Batch 112/764, Train Loss: 0.0003\n",
            "Epoch 19/20, Batch 120/764, Train Loss: 0.0087\n",
            "Epoch 19/20, Batch 128/764, Train Loss: 0.0004\n",
            "Epoch 19/20, Batch 136/764, Train Loss: 0.0010\n",
            "Epoch 19/20, Batch 144/764, Train Loss: 0.0070\n",
            "Epoch 19/20, Batch 152/764, Train Loss: 0.0010\n",
            "Epoch 19/20, Batch 160/764, Train Loss: 0.0026\n",
            "Epoch 19/20, Batch 168/764, Train Loss: 0.1745\n",
            "Epoch 19/20, Batch 176/764, Train Loss: 0.0045\n",
            "Epoch 19/20, Batch 184/764, Train Loss: 0.0010\n",
            "Epoch 19/20, Batch 192/764, Train Loss: 0.0121\n",
            "Epoch 19/20, Batch 200/764, Train Loss: 0.0011\n",
            "Epoch 19/20, Batch 208/764, Train Loss: 0.0167\n",
            "Epoch 19/20, Batch 216/764, Train Loss: 0.0012\n",
            "Epoch 19/20, Batch 224/764, Train Loss: 0.0002\n",
            "Epoch 19/20, Batch 232/764, Train Loss: 0.0014\n",
            "Epoch 19/20, Batch 240/764, Train Loss: 0.0091\n",
            "Epoch 19/20, Batch 248/764, Train Loss: 0.0004\n",
            "Epoch 19/20, Batch 256/764, Train Loss: 0.0007\n",
            "Epoch 19/20, Batch 264/764, Train Loss: 0.0024\n",
            "Epoch 19/20, Batch 272/764, Train Loss: 0.0425\n",
            "Epoch 19/20, Batch 280/764, Train Loss: 0.0002\n",
            "Epoch 19/20, Batch 288/764, Train Loss: 0.0087\n",
            "Epoch 19/20, Batch 296/764, Train Loss: 0.0987\n",
            "Epoch 19/20, Batch 304/764, Train Loss: 0.0003\n",
            "Epoch 19/20, Batch 312/764, Train Loss: 0.0056\n",
            "Epoch 19/20, Batch 320/764, Train Loss: 0.0006\n",
            "Epoch 19/20, Batch 328/764, Train Loss: 0.0033\n",
            "Epoch 19/20, Batch 336/764, Train Loss: 0.0011\n",
            "Epoch 19/20, Batch 344/764, Train Loss: 0.1713\n",
            "Epoch 19/20, Batch 352/764, Train Loss: 0.0166\n",
            "Epoch 19/20, Batch 360/764, Train Loss: 0.0013\n",
            "Epoch 19/20, Batch 368/764, Train Loss: 0.0002\n",
            "Epoch 19/20, Batch 376/764, Train Loss: 0.0007\n",
            "Epoch 19/20, Batch 384/764, Train Loss: 0.0001\n",
            "Epoch 19/20, Batch 392/764, Train Loss: 0.0007\n",
            "Epoch 19/20, Batch 400/764, Train Loss: 0.0001\n",
            "Epoch 19/20, Batch 408/764, Train Loss: 0.0006\n",
            "Epoch 19/20, Batch 416/764, Train Loss: 0.0232\n",
            "Epoch 19/20, Batch 424/764, Train Loss: 0.3443\n",
            "Epoch 19/20, Batch 432/764, Train Loss: 0.0017\n",
            "Epoch 19/20, Batch 440/764, Train Loss: 0.0478\n",
            "Epoch 19/20, Batch 448/764, Train Loss: 0.0193\n",
            "Epoch 19/20, Batch 456/764, Train Loss: 0.0004\n",
            "Epoch 19/20, Batch 464/764, Train Loss: 0.7777\n",
            "Epoch 19/20, Batch 472/764, Train Loss: 0.0003\n",
            "Epoch 19/20, Batch 480/764, Train Loss: 0.0024\n",
            "Epoch 19/20, Batch 488/764, Train Loss: 0.0006\n",
            "Epoch 19/20, Batch 496/764, Train Loss: 0.0696\n",
            "Epoch 19/20, Batch 504/764, Train Loss: 0.0030\n",
            "Epoch 19/20, Batch 512/764, Train Loss: 0.0036\n",
            "Epoch 19/20, Batch 520/764, Train Loss: 1.1232\n",
            "Epoch 19/20, Batch 528/764, Train Loss: 0.0018\n",
            "Epoch 19/20, Batch 536/764, Train Loss: 0.0069\n",
            "Epoch 19/20, Batch 544/764, Train Loss: 0.0003\n",
            "Epoch 19/20, Batch 552/764, Train Loss: 0.0008\n",
            "Epoch 19/20, Batch 560/764, Train Loss: 0.0023\n",
            "Epoch 19/20, Batch 568/764, Train Loss: 0.0114\n",
            "Epoch 19/20, Batch 576/764, Train Loss: 0.0025\n",
            "Epoch 19/20, Batch 584/764, Train Loss: 0.2076\n",
            "Epoch 19/20, Batch 592/764, Train Loss: 0.0006\n",
            "Epoch 19/20, Batch 600/764, Train Loss: 0.0001\n",
            "Epoch 19/20, Batch 608/764, Train Loss: 0.0003\n",
            "Epoch 19/20, Batch 616/764, Train Loss: 0.0028\n",
            "Epoch 19/20, Batch 624/764, Train Loss: 0.0005\n",
            "Epoch 19/20, Batch 632/764, Train Loss: 0.0045\n",
            "Epoch 19/20, Batch 640/764, Train Loss: 0.0011\n",
            "Epoch 19/20, Batch 648/764, Train Loss: 0.1949\n",
            "Epoch 19/20, Batch 656/764, Train Loss: 0.0013\n",
            "Epoch 19/20, Batch 664/764, Train Loss: 0.0025\n",
            "Epoch 19/20, Batch 672/764, Train Loss: 0.0071\n",
            "Epoch 19/20, Batch 680/764, Train Loss: 0.0010\n",
            "Epoch 19/20, Batch 688/764, Train Loss: 0.0010\n",
            "Epoch 19/20, Batch 696/764, Train Loss: 0.0007\n",
            "Epoch 19/20, Batch 704/764, Train Loss: 0.3165\n",
            "Epoch 19/20, Batch 712/764, Train Loss: 0.0375\n",
            "Epoch 19/20, Batch 720/764, Train Loss: 0.0003\n",
            "Epoch 19/20, Batch 728/764, Train Loss: 0.2389\n",
            "Epoch 19/20, Batch 736/764, Train Loss: 0.0053\n",
            "Epoch 19/20, Batch 744/764, Train Loss: 0.0087\n",
            "Epoch 19/20, Batch 752/764, Train Loss: 0.3920\n",
            "Epoch 19/20, Batch 760/764, Train Loss: 0.0012\n",
            "========================================================================================\n",
            "Epoch 19/20, Val Loss: 0.3392, Val Accuracy: 0.9275, Val F1: 0.9343, Best Accuracy: 0.9443\n",
            "========================================================================================\n",
            "Epoch 20/20, Batch 8/764, Train Loss: 0.1443\n",
            "Epoch 20/20, Batch 16/764, Train Loss: 0.0132\n",
            "Epoch 20/20, Batch 24/764, Train Loss: 0.0018\n",
            "Epoch 20/20, Batch 32/764, Train Loss: 0.0010\n",
            "Epoch 20/20, Batch 40/764, Train Loss: 0.0007\n",
            "Epoch 20/20, Batch 48/764, Train Loss: 0.0005\n",
            "Epoch 20/20, Batch 56/764, Train Loss: 0.0171\n",
            "Epoch 20/20, Batch 64/764, Train Loss: 0.0005\n",
            "Epoch 20/20, Batch 72/764, Train Loss: 0.0006\n",
            "Epoch 20/20, Batch 80/764, Train Loss: 0.0007\n",
            "Epoch 20/20, Batch 88/764, Train Loss: 0.0044\n",
            "Epoch 20/20, Batch 96/764, Train Loss: 0.0003\n",
            "Epoch 20/20, Batch 104/764, Train Loss: 0.0005\n",
            "Epoch 20/20, Batch 112/764, Train Loss: 0.0006\n",
            "Epoch 20/20, Batch 120/764, Train Loss: 0.0006\n",
            "Epoch 20/20, Batch 128/764, Train Loss: 0.5516\n",
            "Epoch 20/20, Batch 136/764, Train Loss: 0.0034\n",
            "Epoch 20/20, Batch 144/764, Train Loss: 0.2186\n",
            "Epoch 20/20, Batch 152/764, Train Loss: 0.0067\n",
            "Epoch 20/20, Batch 160/764, Train Loss: 0.0022\n",
            "Epoch 20/20, Batch 168/764, Train Loss: 0.0090\n",
            "Epoch 20/20, Batch 176/764, Train Loss: 0.0030\n",
            "Epoch 20/20, Batch 184/764, Train Loss: 0.0024\n",
            "Epoch 20/20, Batch 192/764, Train Loss: 0.0046\n",
            "Epoch 20/20, Batch 200/764, Train Loss: 0.0067\n",
            "Epoch 20/20, Batch 208/764, Train Loss: 0.0036\n",
            "Epoch 20/20, Batch 216/764, Train Loss: 0.0272\n",
            "Epoch 20/20, Batch 224/764, Train Loss: 0.0097\n",
            "Epoch 20/20, Batch 232/764, Train Loss: 0.0016\n",
            "Epoch 20/20, Batch 240/764, Train Loss: 0.0025\n",
            "Epoch 20/20, Batch 248/764, Train Loss: 0.0043\n",
            "Epoch 20/20, Batch 256/764, Train Loss: 0.0088\n",
            "Epoch 20/20, Batch 264/764, Train Loss: 0.0008\n",
            "Epoch 20/20, Batch 272/764, Train Loss: 1.1583\n",
            "Epoch 20/20, Batch 280/764, Train Loss: 0.0039\n",
            "Epoch 20/20, Batch 288/764, Train Loss: 0.0452\n",
            "Epoch 20/20, Batch 296/764, Train Loss: 0.0008\n",
            "Epoch 20/20, Batch 304/764, Train Loss: 0.0034\n",
            "Epoch 20/20, Batch 312/764, Train Loss: 0.0003\n",
            "Epoch 20/20, Batch 320/764, Train Loss: 0.0001\n",
            "Epoch 20/20, Batch 328/764, Train Loss: 0.6494\n",
            "Epoch 20/20, Batch 336/764, Train Loss: 0.0013\n",
            "Epoch 20/20, Batch 344/764, Train Loss: 0.0318\n",
            "Epoch 20/20, Batch 352/764, Train Loss: 0.0004\n",
            "Epoch 20/20, Batch 360/764, Train Loss: 0.0005\n",
            "Epoch 20/20, Batch 368/764, Train Loss: 0.0015\n",
            "Epoch 20/20, Batch 376/764, Train Loss: 0.0006\n",
            "Epoch 20/20, Batch 384/764, Train Loss: 0.0348\n",
            "Epoch 20/20, Batch 392/764, Train Loss: 0.0017\n",
            "Epoch 20/20, Batch 400/764, Train Loss: 0.0006\n",
            "Epoch 20/20, Batch 408/764, Train Loss: 0.0023\n",
            "Epoch 20/20, Batch 416/764, Train Loss: 0.0019\n",
            "Epoch 20/20, Batch 424/764, Train Loss: 0.0942\n",
            "Epoch 20/20, Batch 432/764, Train Loss: 0.0002\n",
            "Epoch 20/20, Batch 440/764, Train Loss: 0.0040\n",
            "Epoch 20/20, Batch 448/764, Train Loss: 0.1085\n",
            "Epoch 20/20, Batch 456/764, Train Loss: 0.0045\n",
            "Epoch 20/20, Batch 464/764, Train Loss: 0.0031\n",
            "Epoch 20/20, Batch 472/764, Train Loss: 0.0009\n",
            "Epoch 20/20, Batch 480/764, Train Loss: 0.1982\n",
            "Epoch 20/20, Batch 488/764, Train Loss: 0.0186\n",
            "Epoch 20/20, Batch 496/764, Train Loss: 0.0014\n",
            "Epoch 20/20, Batch 504/764, Train Loss: 0.0035\n",
            "Epoch 20/20, Batch 512/764, Train Loss: 0.0011\n",
            "Epoch 20/20, Batch 520/764, Train Loss: 0.0029\n",
            "Epoch 20/20, Batch 528/764, Train Loss: 0.0003\n",
            "Epoch 20/20, Batch 536/764, Train Loss: 0.0001\n",
            "Epoch 20/20, Batch 544/764, Train Loss: 0.0052\n",
            "Epoch 20/20, Batch 552/764, Train Loss: 0.0002\n",
            "Epoch 20/20, Batch 560/764, Train Loss: 0.0021\n",
            "Epoch 20/20, Batch 568/764, Train Loss: 0.0002\n",
            "Epoch 20/20, Batch 576/764, Train Loss: 0.0783\n",
            "Epoch 20/20, Batch 584/764, Train Loss: 0.0001\n",
            "Epoch 20/20, Batch 592/764, Train Loss: 0.0916\n",
            "Epoch 20/20, Batch 600/764, Train Loss: 0.0003\n",
            "Epoch 20/20, Batch 608/764, Train Loss: 0.0004\n",
            "Epoch 20/20, Batch 616/764, Train Loss: 0.0146\n",
            "Epoch 20/20, Batch 624/764, Train Loss: 0.0228\n",
            "Epoch 20/20, Batch 632/764, Train Loss: 0.0285\n",
            "Epoch 20/20, Batch 640/764, Train Loss: 0.0335\n",
            "Epoch 20/20, Batch 648/764, Train Loss: 0.1627\n",
            "Epoch 20/20, Batch 656/764, Train Loss: 0.2080\n",
            "Epoch 20/20, Batch 664/764, Train Loss: 0.8721\n",
            "Epoch 20/20, Batch 672/764, Train Loss: 0.2910\n",
            "Epoch 20/20, Batch 680/764, Train Loss: 0.0018\n",
            "Epoch 20/20, Batch 688/764, Train Loss: 0.0024\n",
            "Epoch 20/20, Batch 696/764, Train Loss: 0.1509\n",
            "Epoch 20/20, Batch 704/764, Train Loss: 0.0016\n",
            "Epoch 20/20, Batch 712/764, Train Loss: 0.0015\n",
            "Epoch 20/20, Batch 720/764, Train Loss: 0.0007\n",
            "Epoch 20/20, Batch 728/764, Train Loss: 0.0002\n",
            "Epoch 20/20, Batch 736/764, Train Loss: 0.0003\n",
            "Epoch 20/20, Batch 744/764, Train Loss: 0.0002\n",
            "Epoch 20/20, Batch 752/764, Train Loss: 0.0021\n",
            "Epoch 20/20, Batch 760/764, Train Loss: 0.0016\n",
            "========================================================================================\n",
            "Epoch 20/20, Val Loss: 0.3189, Val Accuracy: 0.9260, Val F1: 0.9292, Best Accuracy: 0.9443\n",
            "========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After ten epochs, 2000 samples in the dataset, I got the best-case accuracy of 82.3% with the base model.\n",
        "\n",
        "After two epochs, full dataset, I got 0.8687 Val Accuracy with the small model...\n",
        "\n",
        "\n",
        "##Evaluating the Model\n",
        "As a last step, we will test our trained model on an unseen test set. To achieve this, import the model with the best accuracy, which was saved during training, and pass it to the evaluate() method, along with the test dataset."
      ],
      "metadata": {
        "id": "GU2hXBHNHAB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = torch.load('best_model.pt', weights_only=False) ## since we wrote the pickle file\n",
        "\n",
        "# Create a new instance of the model and load the state dictionary\n",
        "num_labels = 10\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "model.load_state_dict(state_dict)\n",
        "import time\n",
        "t1 = time.perf_counter()\n",
        "\n",
        "_, _, _, all_labels, all_preds = evaluate(model, test_loader, device)\n",
        "\n",
        "t2 = time.perf_counter()\n",
        "print('time taken to run prediction: ',t2-t1)\n",
        "\n",
        "print(classification_report(all_labels, all_preds))\n",
        "print(accuracy_score(all_labels, all_preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "lBhZYjWoHDuQ",
        "outputId": "b102a269-4c9b-4c10-9de3-e4220f7d6438"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-dc93f3999026>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load('best_model.pt')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.64      0.72        39\n",
            "           1       1.00      0.89      0.94        18\n",
            "           2       0.73      0.88      0.80        34\n",
            "           3       0.97      0.91      0.94        33\n",
            "           4       1.00      0.76      0.86        25\n",
            "           5       0.73      0.89      0.80        27\n",
            "           6       0.94      0.94      0.94        18\n",
            "           7       0.83      0.92      0.87        37\n",
            "           8       0.96      0.90      0.93        30\n",
            "           9       0.74      0.82      0.78        39\n",
            "\n",
            "    accuracy                           0.85       300\n",
            "   macro avg       0.87      0.86      0.86       300\n",
            "weighted avg       0.86      0.85      0.85       300\n",
            "\n",
            "0.8466666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We achieved an accuracy of 86.30% on the test set. Feel free to experiment by adding or removing dense and dropout layers and adjusting the learning rate to explore potential improvements.\n",
        "\n",
        "I hope you found this tutorial helpful. You should now be able to fine-tune the OpenAI Whisper model from Hugging Face in your PyTorch scripts. If you have any questions or feedback, please feel free to leave them in the comments, and I will do my best to respond promptly."
      ],
      "metadata": {
        "id": "hBdmPUlEHHdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Save model to local drive, and to hugging face"
      ],
      "metadata": {
        "id": "OyS1WVLSkZVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#save model\n",
        "torch.save(model.state_dict(),\"/content/drive/MyDrive/UrbanSound8K/state_dict_20epochs\" )\n",
        "torch.save(model,\"/content/drive/MyDrive/UrbanSound8K/best_model_20epochs.pt\")\n",
        "torch.save(model,\"/content/drive/MyDrive/UrbanSound8K/best_model_20epochs.bin\")"
      ],
      "metadata": {
        "id": "_Z-OjUF9YQ3j"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#push to huggingface\n",
        "#trainer.push_to_hub(\"dedgington/best_model.pt\")\n",
        "#I did this by hand by using git to the hugging-face repo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "GqQl7UwVN-EJ",
        "outputId": "cf7483e1-b7f8-4f0e-b879-46ba8a42b832"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'trainer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-09545fe8812f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#push to huggingface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dedgington/best_model.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using .bin files\n",
        ".bin files in the context of PyTorch models typically represent the model's weights saved in a binary format. Here's what you need to know:\n",
        "####Understanding .bin files:\n",
        "#####Format:\n",
        "These files contain the raw weights and biases of the model, stored in a binary format for efficiency.\n",
        "#####Origin:\n",
        "Often, .bin files are associated with models from Hugging Face's Transformers library.\n",
        "#####Loading:\n",
        "To load a .bin model, you'll typically use the from_pretrained() method within the relevant Transformers class.\n",
        "#####Example:"
      ],
      "metadata": {
        "id": "8jZggGHAjbSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"path/to/model\", from_flax=True)"
      ],
      "metadata": {
        "id": "5q0YVoeTj7Y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Alternatives to .bin:\n",
        "\n",
        "*  .pt or .pth: These are the traditional formats for saving PyTorch models. They save the entire model object, including the architecture, weights, and optimizer state.\n",
        "*   .safetensors: This is a newer format designed to be more secure and portable.\n",
        "\n",
        "\n",
        "Converting .bin to .pt:\n",
        "If you need to convert a .bin file to a .pt file, you can do so using the following steps:"
      ],
      "metadata": {
        "id": "ecocEBx7j-U9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "# Load the model from the .bin file\n",
        "model = AutoModelForCausalLM.from_pretrained(\"path/to/model\", from_flax=True)\n",
        "\n",
        "# Save the model as a .pt file\n",
        "torch.save(model.state_dict(), \"path/to/model.pt\")"
      ],
      "metadata": {
        "id": "FGU9V3pBjpFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Serialize the state disctionary and save it to the specified file ('model.bin' in this case)."
      ],
      "metadata": {
        "id": "Tb7TBPDGtU1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model.bin')"
      ],
      "metadata": {
        "id": "1JSHkAhGtkdv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}